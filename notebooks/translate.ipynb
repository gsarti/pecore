{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install transformers unbabel-comet sacrebleu statsmodels nbformat plotly bitsandbytes\n",
    "#%pip uninstall -y torchvision\n",
    "%pip uninstall -y inseq\n",
    "%pip install git+https://github.com/inseq-team/inseq.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "#flores = datasets.load_dataset(\"gsarti/flores_101\", \"all\", split=\"devtest\")\n",
    "#iwslt = datasets.load_dataset(\"gsarti/iwslt2017_context\", \"iwslt2017-en-fr\", split=\"test\")\n",
    "scat = datasets.load_dataset(\"inseq/scat\", split=\"test\", verification_mode=\"no_checks\")#, download_mode=\"force_redownload\")\n",
    "\n",
    "ds = {\"scat\": scat}#\"flores\": flores, \"iwslt17\": iwslt, \"scat\": scat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from random import random\n",
    "\n",
    "# Used in dataset preprocessing to sample the number of context sentences to include\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "    'Counter that remembers the order elements are first encountered'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '%s(%r)' % (self.__class__.__name__, OrderedDict(self))\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return self.__class__, (OrderedDict(self),)\n",
    "\n",
    "def get_preprocess_dataset(ctx_size, dataset=\"flores\", src_lang=\"eng\"):\n",
    "    def preprocess_dataset_seq(examples):\n",
    "        if dataset == \"flores\":\n",
    "            inputs = examples[f\"sentence_{src_lang[:3]}\"]\n",
    "            n_previous = [i for _, v in OrderedCounter(examples[\"URL\"]).items() for i in range(v)]\n",
    "        elif dataset == \"iwslt17\":\n",
    "            inputs = [ex[src_lang[:2]] for ex in examples[\"translation\"]]\n",
    "            n_previous = [i for _, v in OrderedCounter(examples[\"doc_id\"]).items() for i in range(v)]\n",
    "        else:\n",
    "            raise ValueError(f\"Not available: {dataset}\")\n",
    "        n_contexts = [ctx_size for _ in range(len(inputs))]\n",
    "        n_contexts = [min(n_contexts[idx], n_previous[idx]) for idx in range(len(n_contexts))]\n",
    "        context_inputs = []\n",
    "        for idx in range(len(inputs)):\n",
    "            if n_contexts[idx] > 0:\n",
    "                ctx = \" \".join(inputs[idx - n_contexts[idx]:idx])\n",
    "                context_inputs.append(f\"{ctx}<brk> {inputs[idx]}\")\n",
    "            else:\n",
    "                context_inputs.append(inputs[idx])\n",
    "        return {\"sentence\": context_inputs}\n",
    "    \n",
    "    def preprocess_dataset_merged(examples):\n",
    "        if dataset == \"scat\":\n",
    "            inputs = examples[src_lang[:2]]\n",
    "            contexts = examples[f\"context_{src_lang[:2]}\"]\n",
    "        context_inputs = []\n",
    "        for idx in range(len(inputs)):\n",
    "            if ctx_size > 0:\n",
    "                context_inputs.append(f\"{contexts[idx]}<brk> {inputs[idx]}\")\n",
    "            else:\n",
    "                context_inputs.append(inputs[idx])\n",
    "        return {\"sentence\": context_inputs}\n",
    "\n",
    "    if dataset in [\"flores\", \"iwslt17\"]:\n",
    "        return preprocess_dataset_seq\n",
    "    elif dataset in [\"scat\"]:\n",
    "        return preprocess_dataset_merged\n",
    "\n",
    "def encode(examples, tokenizer):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_path = \"translations/translations\"\n",
    "\n",
    "def translate(cwd, ctx, model_type, dataset=\"flores\", src_lang=\"eng\", use_context=True, has_lang_tag=False, model_name: str = None):\n",
    "    if model_name is None:\n",
    "        model_id = f\"{model_type}-ctx{ctx}-cwd{cwd}\"\n",
    "        model_name = f\"context-mt/iwslt17-{model_id}-en-fr\"\n",
    "    else:\n",
    "        model_id = model_type\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    if not has_lang_tag:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    else:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, src_lang=\"en_XX\", tgt_lang=\"fr_XX\")\n",
    "    preproc_fn = get_preprocess_dataset(ctx if use_context else 0, dataset=dataset, src_lang=src_lang)\n",
    "    data_preproc = ds[dataset].map(preproc_fn, batched=True, batch_size = 2000, remove_columns=ds[dataset].column_names)\n",
    "    data_tokenized = data_preproc.map(lambda x: encode(x, tok), batched=True, remove_columns=[\"sentence\"])\n",
    "    data_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    dataloader = torch.utils.data.DataLoader(data_tokenized, batch_size=8 if \"marian-small\" in model_type else 4 if \"marian-big\" in model_type else 1)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.eval().to(device)\n",
    "    print(f\"Translating...\")\n",
    "    with open(os.path.join(base_path, f\"{dataset}-{model_id}{'-noctx' if not use_context else ''}.txt\"), 'a') as f:\n",
    "        for i, batch in enumerate(tqdm(dataloader)):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            if not has_lang_tag:\n",
    "                out = model.generate(**batch)\n",
    "            else:\n",
    "                out = model.generate(**batch, forced_bos_token_id=tok.lang_code_to_id[\"fr_XX\"])\n",
    "            if use_context:\n",
    "                translations = tok.batch_decode(out.to(\"cpu\"), skip_special_tokens=False)\n",
    "                translations = [t.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").replace(\"fr_XX\", \"\").strip() for t in translations]\n",
    "            else:\n",
    "                translations = tok.batch_decode(out.to(\"cpu\"), skip_special_tokens=True)\n",
    "            if i == 0:\n",
    "              print(translations[:2])\n",
    "            for trans in translations:\n",
    "                f.write(trans + \"\\n\")\n",
    "\n",
    "default_models = {\n",
    "    \"marian-small\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "    \"marian-big\": \"Helsinki-NLP/opus-mt-tc-big-en-fr\",\n",
    "    \"mbart50-1toM\": \"facebook/mbart-large-50-one-to-many-mmt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cwd in range(6):\n",
    "    for model_type in [\"marian-small\", \"marian-big\"]:\n",
    "        translate(cwd, 4, model_type)\n",
    "\n",
    "for ctx in range(0, 11, 2):\n",
    "    for model_type in [\"marian-small\", \"marian-big\"]:\n",
    "        translate(0, ctx, model_type)\n",
    "\n",
    "for cwd in range(6):\n",
    "    for model_type in [\"marian-small\", \"marian-big\"]:\n",
    "        translate(cwd, 4, model_type, dataset=\"iwslt17\")\n",
    "\n",
    "for ctx in range(6, 11, 2):\n",
    "    for model_type in [\"marian-small\", \"marian-big\"]:\n",
    "        translate(0, ctx, model_type, dataset=\"iwslt17\")\n",
    "\n",
    "for cwd in range(6):\n",
    "    for model_type in [\"marian-small\", \"marian-big\"]:\n",
    "        translate(cwd, 4, model_type, dataset=\"scat\")\n",
    "\n",
    "for ctx in range(0, 11, 2):\n",
    "    for model_type in [\"marian-small\", \"marian-big\"]:\n",
    "        translate(0, ctx, model_type, dataset=\"scat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ctx, cwd in zip([4], [1]):#[0, 4], [0, 1]):\n",
    "    #translate(cwd, ctx, \"mbart50-1toM\", has_lang_tag=True)\n",
    "    translate(cwd, ctx, \"mbart50-1toM\", has_lang_tag=True, dataset=\"iwslt17\")\n",
    "    #translate(cwd, ctx, \"mbart50-1toM\", has_lang_tag=True, dataset=\"scat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type, model_name in default_models.items():\n",
    "    if model_type == \"mbart50-1toM\":\n",
    "        for dataset in [\"scat\"]:#[\"flores\"]:#ds.keys():\n",
    "            translate(0, 0, model_type, has_lang_tag=model_type == \"mbart50-1toM\", use_context=False, dataset=dataset, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate(1, 4, \"marian-big-scat\", dataset=\"scat\", model_name=\"context-mt/scat-marian-big-ctx4-cwd1-en-fr\")\n",
    "#translate(1, 4, \"marian-big-scat\", dataset=\"flores\", model_name=\"context-mt/scat-marian-big-ctx4-cwd1-en-fr\")\n",
    "translate(1, 4, \"marian-small-scat\", dataset=\"scat\", model_name=\"context-mt/scat-marian-small-ctx4-cwd1-en-fr\")\n",
    "translate(1, 4, \"marian-small-scat\", dataset=\"flores\", model_name=\"context-mt/scat-marian-small-ctx4-cwd1-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"translations/translations_noctx\"\n",
    "\n",
    "#for cwd in range(6):\n",
    "#    for model_type in [\"marian-small\", \"marian-big\"]:\n",
    "#        translate(cwd, 4, model_type, dataset=\"scat\", use_context=False)\n",
    "#\n",
    "#for ctx in range(0, 11, 2):\n",
    "#    for model_type in [\"marian-small\", \"marian-big\"]:\n",
    "#        translate(0, ctx, model_type, dataset=\"scat\", use_context=False)\n",
    "#\n",
    "#for ctx, cwd in zip([4], [1]):#[0, 4], [0, 1]):\n",
    "#    translate(cwd, ctx, \"mbart50-1toM\", has_lang_tag=True, use_context=False, dataset=\"scat\")\n",
    "\n",
    "#translate(1, 4, \"marian-big-scat\", dataset=\"scat\", use_context=True, model_name=\"context-mt/scat-marian-big-ctx4-cwd1-en-fr\")\n",
    "#translate(1, 4, \"marian-small-scat\", dataset=\"scat\", use_context=True, model_name=\"context-mt/scat-marian-small-ctx4-cwd1-en-fr\")\n",
    "#translate(1, 4, \"marian-big-scat\", dataset=\"scat\", use_context=False, model_name=\"context-mt/scat-marian-big-ctx4-cwd1-en-fr\")\n",
    "translate(1, 4, \"marian-small-scat\", dataset=\"scat\", use_context=False, model_name=\"context-mt/scat-marian-small-ctx4-cwd1-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate(0, 4, \"marian-small-scat-target\", dataset=\"scat\", use_context=True, model_name=\"context-mt/scat-marian-small-target-ctx4-cwd0-en-fr\")\n",
    "#translate(0, 4, \"marian-big-scat-target\", dataset=\"scat\", use_context=True, model_name=\"context-mt/scat-marian-big-target-ctx4-cwd0-en-fr\")\n",
    "translate(0, 4, \"mbart50-1toM-scat-target\", dataset=\"scat\", use_context=True, has_lang_tag=True, model_name=\"context-mt/scat-mbart50-1toM-target-ctx4-cwd0-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for use_context in [True, False]:\n",
    "    base_path = f\"translations/translations{'_noctx' if not use_context else ''}\"\n",
    "    #translate(1, 4, \"marian-small-scat\", dataset=\"scat\", use_context=use_context, model_name=\"context-mt/scat-marian-small-ctx4-cwd1-en-fr\")\n",
    "    #translate(1, 4, \"marian-big-scat\", dataset=\"scat\", use_context=use_context, model_name=\"context-mt/scat-marian-big-ctx4-cwd1-en-fr\")\n",
    "    translate(1, 4, \"mbart50-1toM-scat\", dataset=\"scat\", use_context=use_context, has_lang_tag=True, model_name=\"context-mt/scat-mbart50-1toM-ctx4-cwd1-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"translations/translations_noctx\"\n",
    "#translate(0, 4, \"marian-small-scat-target\", dataset=\"scat\", use_context=False, model_name=\"context-mt/scat-marian-small-target-ctx4-cwd0-en-fr\")\n",
    "#translate(0, 4, \"marian-big-scat-target\", dataset=\"scat\", use_context=False, model_name=\"context-mt/scat-marian-big-target-ctx4-cwd0-en-fr\")\n",
    "#translate(0, 4, \"mbart50-1toM-scat-target\", dataset=\"scat\", use_context=False, has_lang_tag=True, model_name=\"context-mt/scat-mbart50-1toM-target-ctx4-cwd0-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import re\n",
    "\n",
    "def get_aligned_gender_annotations(ref_text, contrast_ref_text, mt_text) -> List[Tuple[str, str]]:\n",
    "    \"\"\" Returns a list of 0s and 1s, where 0 means that the word is not in the MT output and 1 means that it is. \"\"\"\n",
    "    ref_tok = re.findall(r'\\w+\\b', ref_text)\n",
    "    contrast_ref_tok = re.findall(r'\\w+\\b', contrast_ref_text)\n",
    "    if not isinstance(mt_text, str):\n",
    "        return [0]\n",
    "    mt_tok = [x.lower() for x in re.findall(r'\\w+\\b', mt_text)]\n",
    "    keywords = [ref for ref, con in zip(ref_tok, contrast_ref_tok) if ref != con]\n",
    "    out = []\n",
    "    for kw in keywords:\n",
    "        if kw.lower() not in mt_tok:\n",
    "            out += [0]\n",
    "        else:\n",
    "            out += [1]\n",
    "            mt_tok.remove(kw.lower())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "model = load_from_checkpoint(model_path)\n",
    "\n",
    "import logging\n",
    "loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "for logger in loggers:\n",
    "    logger.setLevel(logging.WARNING)\n",
    "\n",
    "def evaluate(cwd = 0, ctx = 0, model_type = \"\", dataset=\"flores\", src_lang=\"eng\", tgt_lang=\"fra\", metric=\"bleu\", use_context=True, model_name = None, use_target_context = False):\n",
    "    if model_name is None:\n",
    "        model_id = f\"{model_type}-ctx{ctx}-cwd{cwd}\"\n",
    "    else:\n",
    "        model_id = model_type\n",
    "    if dataset == \"flores\":\n",
    "        src = flores[f\"sentence_{src_lang[:3]}\"]\n",
    "        refs = flores[f\"sentence_{tgt_lang[:3]}\"]\n",
    "    elif dataset == \"iwslt17\":\n",
    "        src = [ex[src_lang[:2]] for ex in iwslt[\"translation\"]]\n",
    "        refs = [ex[tgt_lang[:2]] for ex in iwslt[\"translation\"]]\n",
    "    elif dataset == \"scat\":\n",
    "        src = scat[src_lang[:2]]\n",
    "        refs = scat[tgt_lang[:2]]\n",
    "    base_path = \"translations/translations\" if use_context else \"translations/translations_noctx\"\n",
    "    with open(os.path.join(base_path, f\"{dataset}-{model_id}{'' if use_context else '-noctx'}.txt\"), 'r') as f:\n",
    "        sys = f.readlines()\n",
    "    if use_target_context:\n",
    "        # Remove target context for evaluation\n",
    "        sys = [s.split(\"<brk>\")[1].strip() if \"<brk>\" in s else s for s in sys]\n",
    "    if metric == \"comet\":\n",
    "        comet_out = model.predict([{\"src\": s, \"mt\": m, \"ref\": r} for s, m, r in zip(src, sys, refs)], batch_size=8, gpus=1)\n",
    "        print(dataset, f\"{model_id}{'' if use_context else '-noctx'}\", \"COMET\", comet_out.system_score)\n",
    "    if metric == \"bleu\":\n",
    "        bleu = BLEU()\n",
    "        print(dataset, f\"{model_id}{'' if use_context else '-noctx'}\", bleu.corpus_score(sys, [refs]))\n",
    "    if metric == \"accuracy\":\n",
    "        if dataset != \"scat\":\n",
    "            raise ValueError(\"Only scat dataset supports accuracy metric\")\n",
    "        tot_keywords, tot_correct = 0, 0\n",
    "        for curr_ref, curr_ref_contrast, curr_mt in tqdm(zip(refs, scat[f\"contrast_{tgt_lang[:2]}\"], sys), desc=\"Aligned accuracy\", total=len(refs)):\n",
    "            matches = get_aligned_gender_annotations(curr_ref, curr_ref_contrast, curr_mt)\n",
    "            tot_keywords += len(matches)\n",
    "            tot_correct += len([x for x in matches if x == 1])\n",
    "        print(dataset, f\"{model_id}{'' if use_context else '-noctx'}\", \"align_match_acc\", round(tot_correct / tot_keywords, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for use_context in [True]:#[True, False]:\n",
    "    if not use_context:\n",
    "        data = [\"scat\"]\n",
    "    else:\n",
    "        data = [\"iwslt17\", \"flores\", \"scat\"]\n",
    "    for dataset in [\"scat\"]:#data:\n",
    "        for model_type in [\"marian-small\", \"marian-big\", \"mbart50-1toM\"]:\n",
    "            for metric in [\"accuracy\"]:#[\"bleu\", \"comet\"]:\n",
    "                for cwd in range(6):\n",
    "                    evaluate(cwd, 4, model_type, dataset=dataset, metric=metric, use_context=use_context)\n",
    "                print(\"*\" * 20)\n",
    "                for ctx in range(0, 11, 2):\n",
    "                    evaluate(0, ctx, model_type, dataset=dataset, metric=metric, use_context=use_context)\n",
    "            print(\"-\" * 20)\n",
    "        print(\"=\" * 20)\n",
    "        if dataset != \"iwslt17\":\n",
    "            for metric in [\"accuracy\"]:#[\"bleu\", \"comet\"]:\n",
    "                for ctx, cwd in zip([4], [1]):\n",
    "                    evaluate(cwd, ctx, \"mbart50-1toM\", dataset=dataset, metric=metric, use_context=use_context)\n",
    "                print(\"*\" * 20)\n",
    "            print(\"=\" * 20)\n",
    "        if use_context:\n",
    "            for model_type, model_name in default_models.items():\n",
    "                if model_type == \"mbart50-1toM\" and dataset == \"iwslt17\":\n",
    "                    continue\n",
    "                for metric in [\"accuracy\"]:#[\"bleu\", \"comet\"]:\n",
    "                    evaluate(0,0, model_type, dataset=dataset, metric=metric, use_context=use_context, model_name=model_name)\n",
    "                print(\"*\" * 20)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"bleu\", \"comet\", \"accuracy\"]:\n",
    "    for use_context in [True]:#[True, False]:\n",
    "        evaluate(0, 4, \"marian-small-scat-target\", dataset=\"scat\", metric=metric, use_context=use_context, use_target_context=True, model_name=\"context-mt/scat-marian-small-target-ctx4-cwd0-en-fr\")\n",
    "        evaluate(0, 4, \"marian-big-scat-target\", dataset=\"scat\", metric=metric, use_context=use_context, use_target_context=True, model_name=\"context-mt/scat-marian-big-target-ctx4-cwd0-en-fr\")\n",
    "        evaluate(0, 4, \"mbart50-1toM-scat-target\", dataset=\"scat\", metric=metric, use_context=use_context, use_target_context=True, model_name=\"context-mt/scat-mbart50-1toM-target-ctx4-cwd0-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"bleu\", \"comet\", \"accuracy\"]:\n",
    "    for use_context in [False]:#[True, False]:\n",
    "        for model_type, model_name in default_models.items():\n",
    "            evaluate(0, 0, model_type, dataset=\"scat\", metric=metric, use_context=False, use_target_context=False, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"bleu\", \"comet\", \"accuracy\"]:\n",
    "    for use_context in [True]:#[True, False]:\n",
    "        evaluate(1, 4, \"marian-small-scat\", dataset=\"scat\", metric=metric, use_context=use_context, use_target_context=False, model_name=\"context-mt/scat-marian-small-ctx4-cwd1-en-fr\")\n",
    "        evaluate(1, 4, \"marian-big-scat\", dataset=\"scat\", metric=metric, use_context=use_context, use_target_context=False, model_name=\"context-mt/scat-marian-big-ctx4-cwd1-en-fr\")\n",
    "        evaluate(0, 4, \"mbart50-1toM-scat\", dataset=\"scat\", metric=metric, use_context=use_context, use_target_context=False, model_name=\"context-mt/scat-mbart50-1toM-ctx4-cwd1-en-fr\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    "- ~~Compute `comet` scores for all models on the three datasets (already implemented, only need running)~~\n",
    "\n",
    "- Translate with mbart pre and post-finetuning and evaluate its performances on the three datasets.\n",
    "\n",
    "- Produce `noctx` translations setting context to 0 for all available models on the `scat` test set - this will be used as reference output for each model when no additional context is provided.\n",
    "\n",
    "- Extract context reference translation for every example. This is used as input to compute the context sensitivity metrics alongside the contrast sources (see call to `model.attribute` below)\n",
    "\n",
    "- Compute and save context-sensitivity scores for every token in every example in SCAT test set. Settings: P-CXMI, full KL-Div, truncated KL-Div at 90% probability. Format: dict `{\"words\": list of tokens, \"scores\": list of floats}`\n",
    "\n",
    "- Compute Accuracy/AUPRC for various threshold values on the target pronoun for available models, evaluate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "def tokenize(text: str, is_tagged: bool = False):\n",
    "    pattern_nonspace = r\"(<p>|</p>|<hon>|<hoff>|\\S+)\" if is_tagged else r\"(\\S+)\"\n",
    "    pattern_word = r\"(<p>|</p>|<hon>|<hoff>|\\w+)\" if is_tagged else r\"(\\w+)\"\n",
    "    return [x for nonspace in re.split(pattern_nonspace, text) for x in re.split(pattern_word, nonspace) if x.strip()]\n",
    "\n",
    "def tokenize_model(text: str, model):\n",
    "    out = model.encode(text, as_targets=True)\n",
    "    return [x.replace(\"▁\", \"\") for x in out.input_tokens[0] if x not in [\"<pad>\", \"</s>\", \"fr_XX\"]]\n",
    "\n",
    "def get_tokens_with_cue_target_tags(txt_tag: str, txt_clean: str):\n",
    "    untagged_toks = tokenize(txt_clean)\n",
    "    tagged_toks = tokenize(txt_tag, is_tagged=True)\n",
    "    tag_idx, untag_idx = 0, 0\n",
    "    cue_tags = [0 for _ in range(len(untagged_toks))]\n",
    "    target_tags = [0 for _ in range(len(untagged_toks))]\n",
    "    is_cue = False\n",
    "    is_target = False\n",
    "    while tag_idx < len(tagged_toks) and untag_idx < len(untagged_toks):\n",
    "        if tagged_toks[tag_idx] == untagged_toks[untag_idx]:\n",
    "            if is_cue:\n",
    "                cue_tags[untag_idx] = 1\n",
    "            elif is_target:\n",
    "                target_tags[untag_idx] = 1\n",
    "            tag_idx += 1\n",
    "            untag_idx += 1\n",
    "        elif tagged_toks[tag_idx] in  [\"<p>\", \"<hon>\"]:\n",
    "            if tagged_toks[tag_idx] == \"<p>\":\n",
    "                is_target = True\n",
    "            elif tagged_toks[tag_idx] == \"<hon>\":\n",
    "                is_cue = True\n",
    "            tag_idx += 1\n",
    "        elif tagged_toks[tag_idx] in [\"</p>\", \"<hoff>\"]:\n",
    "            if tagged_toks[tag_idx] == \"</p>\":\n",
    "                is_target = False\n",
    "            elif tagged_toks[tag_idx] == \"<hoff>\":\n",
    "                is_cue = False\n",
    "            tag_idx += 1\n",
    "        else:\n",
    "            print(tagged_toks[tag_idx], untagged_toks[untag_idx])\n",
    "            raise ValueError(f\"Something went wrong\\nTagged:{tagged_toks}\\nUntagged:{untagged_toks}\")\n",
    "    return untagged_toks, cue_tags, target_tags\n",
    "\n",
    "def get_subword_alignments(src: str, tgt: str) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Aligns tokens of two whitespace-tokenized strings having the same contents,\n",
    "    but differing in tokenization.\n",
    "    The output is a sequence in the format \"0-0 1-1 2-3 3-2 ...\" corresponding to indices of\n",
    "    aligned tokens between src and tgt\n",
    "    \"\"\"\n",
    "    assert \"\".join(src.split(\" \")) == \"\".join(tgt.split(\" \")), f\"SRC: {''.join(src.split())}\\nTGT: {''.join(tgt.split())}\\n\"\n",
    "    out = []\n",
    "    src_idx = 0\n",
    "    tgt_idx = 0\n",
    "    # Splitting on single space ensures that \"_\" tokens are not lost and alignments are preserved.\n",
    "    src_tok = src.strip().split(\" \")\n",
    "    tgt_tok = tgt.strip().split(\" \")\n",
    "    while src_idx < len(src_tok):\n",
    "        curr_src_tok = src_tok[src_idx]\n",
    "        curr_tgt_tok = tgt_tok[tgt_idx]\n",
    "        if curr_src_tok == curr_tgt_tok:\n",
    "            out.append(f\"{src_idx}-{tgt_idx}\")\n",
    "            src_idx += 1\n",
    "            tgt_idx += 1\n",
    "        elif curr_src_tok in curr_tgt_tok:\n",
    "            out.append(f\"{src_idx}-{tgt_idx}\")\n",
    "            tgt_tok[tgt_idx] = tgt_tok[tgt_idx].replace(curr_src_tok, \"\", 1)\n",
    "            src_idx += 1\n",
    "        elif curr_tgt_tok in curr_src_tok:\n",
    "            out.append(f\"{src_idx}-{tgt_idx}\")\n",
    "            src_tok[src_idx] = src_tok[src_idx].replace(curr_tgt_tok, \"\", 1)\n",
    "            tgt_idx += 1\n",
    "        else:\n",
    "            raise ValueError(f\"ERR: {curr_src_tok} =!= {curr_tgt_tok}\")\n",
    "    out = \" \".join(out)\n",
    "    return [tuple(int(x) for x in pair.split(\"-\")) for pair in out.split()]\n",
    "\n",
    "def propagate_tags(tok_tgt, tags, alignments):\n",
    "    model_tok_cue_tags = [0 for _ in range(len(tok_tgt))]\n",
    "    for tok_idx, word_idx in alignments:\n",
    "        if tags[word_idx] == 1:\n",
    "            model_tok_cue_tags[tok_idx] = 1\n",
    "    return model_tok_cue_tags\n",
    "\n",
    "def get_model_cue_target_tags(tagged, untagged, model):    \n",
    "    model_tokenized = tokenize_model(untagged, model)\n",
    "    untagged_toks, cue_tags, target_tags = get_tokens_with_cue_target_tags(tagged, untagged)\n",
    "    try:\n",
    "        alignments = get_subword_alignments(\" \".join(model_tokenized), \" \".join(untagged_toks))\n",
    "    except AssertionError:\n",
    "        raise ValueError(model_tokenized, untagged_toks)\n",
    "    model_tok_cue_tags = propagate_tags(model_tokenized, cue_tags, alignments)\n",
    "    model_tok_target_tags = propagate_tags(model_tokenized, target_tags, alignments)\n",
    "    return model_tok_cue_tags, model_tok_target_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: scat/sentences\n",
      "Found cached dataset scat (/home/gsarti/.cache/huggingface/datasets/inseq___scat/sentences/0.0.0/d0361f176cca9a1b65c6bf59ea1a94ab2c131b30c80e3bffab4f103c0e9406dd)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "scat = datasets.load_dataset(\"inseq/scat\", split=\"test\", verification_mode=\"no_checks\")#, download_mode=\"force_redownload\")\n",
    "ds = {\"scat\": scat}#\"flores\": flores, \"iwslt17\": iwslt, \"scat\": scat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Tuple\n",
    "import torch\n",
    "from inseq.attr.step_functions import StepFunctionArgs, _get_contrast_output\n",
    "from inseq.data import FeatureAttributionInput\n",
    "from inseq.utils import logits_kl_divergence\n",
    "\n",
    "def kl_div_per_layer_fn(\n",
    "    args: StepFunctionArgs,\n",
    "    contrast_target_prefixes: Optional[FeatureAttributionInput] = None,\n",
    "    contrast_sources: Optional[FeatureAttributionInput] = None,\n",
    "    contrast_targets: Optional[FeatureAttributionInput] = None,\n",
    "    contrast_targets_alignments: Optional[List[List[Tuple[int, int]]]] = None,\n",
    "    top_k: int = 0,\n",
    "    top_p: float = 1.0,\n",
    "    min_tokens_to_keep: int = 1,\n",
    "):\n",
    "    \"\"\"Compute the KL divergence between original and contrastive probabilities at every layer of the model using the\n",
    "    logit lens approach to project intermediate hidden states to logits.\n",
    "    \"\"\"\n",
    "\n",
    "    original_batch = args.attribution_model.formatter.convert_args_to_batch(args)\n",
    "    original_output = args.attribution_model.get_forward_output(\n",
    "        original_batch,\n",
    "        output_hidden_states=True,\n",
    "    )\n",
    "    contrast_output = _get_contrast_output(\n",
    "        args,\n",
    "        contrast_sources=contrast_sources,\n",
    "        contrast_target_prefixes=contrast_target_prefixes,\n",
    "        contrast_targets=contrast_targets,\n",
    "        contrast_targets_alignments=contrast_targets_alignments,\n",
    "        output_hidden_states=True,\n",
    "    )\n",
    "    all_kl_divergences = []\n",
    "    for i in range(len(original_output.decoder_hidden_states) - 1):\n",
    "        original_logits = args.attribution_model.model.lm_head(original_output.decoder_hidden_states[i][:, -1, :])\n",
    "        contrast_logits = args.attribution_model.model.lm_head(contrast_output.decoder_hidden_states[i][:, -1, :])\n",
    "        original_logits = original_logits + args.attribution_model.model.final_logits_bias\n",
    "        contrast_logits = contrast_logits + args.attribution_model.model.final_logits_bias\n",
    "        kl_divergence = logits_kl_divergence(\n",
    "            original_logits=original_logits,\n",
    "            contrast_logits=contrast_logits,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            min_tokens_to_keep=min_tokens_to_keep,\n",
    "        )\n",
    "        all_kl_divergences.append(kl_divergence)\n",
    "    return torch.stack(all_kl_divergences, dim=1)\n",
    "\n",
    "import inseq\n",
    "\n",
    "# Register the function defined above\n",
    "# Since outputs are still probabilities, contiguous tokens can still be aggregated using product\n",
    "inseq.register_step_function(\n",
    "    fn=kl_div_per_layer_fn,\n",
    "    identifier=\"kl_div_per_layer\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-08-04 17:12:38,497 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n",
      "2023-08-04 17:12:38 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2023-08-04 17:12:38 INFO: Using device: cuda\n",
      "2023-08-04 17:12:38 INFO: Loading: tokenize\n",
      "2023-08-04 17:12:40 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import inseq\n",
    "from inseq import AttributionModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from typing import List, Dict, Optional, Callable, Tuple\n",
    "\n",
    "from simalign import SentenceAligner\n",
    "import stanza\n",
    "\n",
    "aligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\")\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', download_method=None)\n",
    "\n",
    "default_models = {\n",
    "    \"marian-small\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
    "    \"marian-big\": \"Helsinki-NLP/opus-mt-tc-big-en-fr\",\n",
    "    \"mbart50-1toM\": \"facebook/mbart-large-50-one-to-many-mmt\"\n",
    "}\n",
    "\n",
    "\n",
    "def get_model_id_and_name(\n",
    "    cwd: int = 1, ctx: int = 4, model_type: str = \"\", dataset: str = \"scat\", model_name: str = None\n",
    ") -> Tuple[str, str]:\n",
    "    if model_name is None:\n",
    "        if not model_type:\n",
    "            raise ValueError(\"Must specify model_type or model_name\")\n",
    "        model_id = f\"{dataset}-{model_type}-ctx{ctx}-cwd{cwd}\"\n",
    "        model_name = f\"context-mt/{model_id}-en-fr\"\n",
    "    else:\n",
    "        model_id = model_type\n",
    "    return model_id, model_name\n",
    "\n",
    "\n",
    "def get_formatted_examples(\n",
    "    model_name: str = None,\n",
    "    force_gen: bool = False,\n",
    "    has_context: bool = True,\n",
    "    has_lang_tag: bool = False,\n",
    "    has_target_context: bool = False,\n",
    "    start_idx: int = None,\n",
    "    max_idx: int = None,\n",
    ") -> List[Dict[str, str]]:\n",
    "    if max_idx is None:\n",
    "        max_idx = len(scat)\n",
    "    if start_idx is None:\n",
    "        start_idx = 0\n",
    "    model = inseq.load_model(model_name, \"saliency\")\n",
    "    generate_kwargs = {}\n",
    "    if has_lang_tag:\n",
    "        model.tokenizer.src_lang = \"en_XX\"\n",
    "        model.tokenizer.tgt_lang = \"fr_XX\"\n",
    "        generate_kwargs[\"forced_bos_token_id\"] = model.tokenizer.lang_code_to_id[\"fr_XX\"]\n",
    "    examples = []\n",
    "    for idx, ex in tqdm(enumerate(scat), total=max_idx):\n",
    "        if idx < start_idx:\n",
    "            continue\n",
    "        if max_idx is not None and idx >= max_idx:\n",
    "            break\n",
    "        if not force_gen:\n",
    "            ctx_tgt = None\n",
    "            if has_context:\n",
    "                contrast_sources = ex[\"context_en\"] + \"<brk> \" + ex[\"en\"]\n",
    "            else:\n",
    "                contrast_sources = ex[\"context_en\"] + \" \" + ex[\"en\"]\n",
    "                ctx_tgt = \" \".join([\n",
    "                    model.generate(s.text, max_new_tokens=128, **generate_kwargs)[0]\n",
    "                    for s in nlp(ex[\"context_en\"]).sentences\n",
    "                ])\n",
    "                decoder_input = model.encode(ctx_tgt, as_targets=True).to(model.device)\n",
    "                generate_kwargs[\"decoder_input_ids\"] = decoder_input.input_ids\n",
    "                if has_lang_tag:\n",
    "                    lang_id_tensor = torch.tensor([model.tokenizer.lang_code_to_id[\"fr_XX\"]]).to(model.device)\n",
    "                    # Prepend the ID tensor to the original tensor along the first dimension (rows)\n",
    "                    generate_kwargs[\"decoder_input_ids\"] = torch.cat((lang_id_tensor.unsqueeze(0), generate_kwargs[\"decoder_input_ids\"]), dim=1)\n",
    "            encoded_sources = model.encode(contrast_sources, as_targets=False).to(model.device)\n",
    "            generation_out = model.model.generate(\n",
    "                input_ids=encoded_sources.input_ids,\n",
    "                attention_mask=encoded_sources.attention_mask,\n",
    "                return_dict_in_generate=True,\n",
    "                **generate_kwargs,\n",
    "            )\n",
    "            encoded_sources = encoded_sources.to(\"cpu\")\n",
    "            if not has_context:\n",
    "                decoder_input = decoder_input.to(\"cpu\")\n",
    "            ctx_gen = model.tokenizer.batch_decode(\n",
    "                generation_out.sequences, skip_special_tokens=False if has_target_context else True\n",
    "            )[0].replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
    "            del generation_out\n",
    "            torch.cuda.empty_cache()\n",
    "            if has_context and has_target_context:\n",
    "                ctx_tgt = ctx_gen.split(\"<brk>\")[0].strip() + \"<brk>\"\n",
    "            start_pos = len(ctx_tgt) if ctx_tgt else 0\n",
    "            ctx_gen = ctx_gen[start_pos:]\n",
    "        tgt = ex[\"fr\"] if force_gen else ctx_gen\n",
    "        ctx_tgt = ex[\"context_fr\"] if force_gen else ctx_tgt\n",
    "        examples.append({\n",
    "            \"src_en\": ex[\"en\"],\n",
    "            \"tgt_fr\": tgt,\n",
    "            \"src_en_ctx\": contrast_sources,\n",
    "            \"tgt_fr_ctx\": ctx_tgt,\n",
    "            \"src_en_with_tags\": ex[\"en_with_tags\"],\n",
    "            \"orig_fr\": ex[\"fr\"],\n",
    "            \"orig_fr_with_tags\": ex[\"fr_with_tags\"],\n",
    "        })\n",
    "        if idx < 3:\n",
    "            print(f\"FULL EXAMPLE: {ex}\")\n",
    "            print(f\"SRC: {ex['en']}\")\n",
    "            print(f\"TGT: {tgt}\")\n",
    "            print(f\"SRC CTX: {contrast_sources}\")\n",
    "            print(f\"TGT CTX: {ctx_tgt}\")\n",
    "    return examples\n",
    "\n",
    "\n",
    "def base_attribute_fn(ex: Dict[str, str], model: AttributionModel, idx: int) -> pd.DataFrame:\n",
    "    out = model.attribute(\n",
    "        ex[\"src_en\"],\n",
    "        ex[\"tgt_fr\"],\n",
    "        attribute_target=True,\n",
    "        step_scores=[\"probability\", \"contrast_prob\", \"pcxmi\", \"kl_divergence\"],\n",
    "        contrast_sources=ex[\"src_en_ctx\"],\n",
    "        contrast_target_prefixes=ex[\"tgt_fr_ctx\"],\n",
    "        show_progress=False,\n",
    "    )\n",
    "    df = pd.DataFrame(out.get_scores_dicts()[0][\"step_scores\"])\n",
    "    df = df.transpose().reset_index().rename(columns={\"level_0\": \"token_idx\", \"level_1\": \"token\"})\n",
    "    df.insert(0, \"example_idx\", idx)\n",
    "    return df\n",
    "\n",
    "\n",
    "def top_p_attribute_fn(ex: Dict[str, str], model: AttributionModel, idx: int) -> pd.DataFrame:\n",
    "    overall_df = None\n",
    "    for top_p in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "        out = model.attribute(\n",
    "            ex[\"src_en\"],\n",
    "            ex[\"tgt_fr\"],\n",
    "            attribute_target=True,\n",
    "            step_scores=[\"kl_divergence\", \"top_p_size\"],\n",
    "            contrast_sources=ex[\"src_en_ctx\"],\n",
    "            contrast_target_prefixes=ex[\"tgt_fr_ctx\"],\n",
    "            show_progress=False,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        df = pd.DataFrame(out.get_scores_dicts()[0][\"step_scores\"])\n",
    "        df = df.transpose().reset_index().rename(columns={\"level_0\": \"token_idx\", \"level_1\": \"token\"})\n",
    "        df.insert(0, \"example_idx\", idx)\n",
    "        if overall_df is None:\n",
    "            overall_df = df.rename(columns={\"kl_divergence\": f\"kl_div_{int(top_p * 100)}\", \"top_p_size\": f\"top_p_size_{int(top_p * 100)}\"})\n",
    "        else:\n",
    "            overall_df[f\"kl_div_{int(top_p * 100)}\"] = df[\"kl_divergence\"]\n",
    "            overall_df[f\"top_p_size_{int(top_p * 100)}\"] = df[\"top_p_size\"]\n",
    "    return overall_df\n",
    "\n",
    "\n",
    "def logit_lens_attribute_fn(ex: Dict[str, str], model: AttributionModel, idx: int) -> pd.DataFrame:\n",
    "    out = model.attribute(\n",
    "        ex[\"src_en\"],\n",
    "        ex[\"tgt_fr\"],\n",
    "        attribute_target=True,\n",
    "        step_scores=[\"kl_div_per_layer\"],\n",
    "        contrast_sources=ex[\"src_en_ctx\"],\n",
    "        contrast_target_prefixes=ex[\"tgt_fr_ctx\"],\n",
    "        show_progress=False,\n",
    "    )\n",
    "    for layer_idx in range(out[0].step_scores[\"kl_div_per_layer\"].shape[0]):\n",
    "        out[0].step_scores[f\"kl_div_l{layer_idx}\"] = out[0].step_scores[\"kl_div_per_layer\"][layer_idx, :]\n",
    "    del out[0].step_scores[\"kl_div_per_layer\"]\n",
    "    df = pd.DataFrame(out.get_scores_dicts()[0][\"step_scores\"])\n",
    "    df = df.transpose().reset_index().rename(columns={\"level_0\": \"token_idx\", \"level_1\": \"token\"})\n",
    "    df.insert(0, \"example_idx\", idx)\n",
    "    return df\n",
    "\n",
    "\n",
    "def input_contributions_attribute_fn(ex: Dict[str, str], model: AttributionModel, idx: int) -> pd.DataFrame:\n",
    "    has_target_context = ex[\"tgt_fr_ctx\"] is not None and pd.notnull(ex[\"tgt_fr_ctx\"])\n",
    "    # Handle missing source context\n",
    "    full_src = ex[\"src_en_ctx\"]\n",
    "    if full_src.startswith(\"<brk>\") or not isinstance(ex[\"tgt_fr\"], str):\n",
    "        print(f\"Skipping example {idx}\")\n",
    "        return None\n",
    "    full_tgt = ex[\"tgt_fr_ctx\"] + \" \" + ex[\"tgt_fr\"].strip() if has_target_context else ex[\"tgt_fr\"].strip()\n",
    "    tgt_fr_ctx_tokens = model.encode(ex[\"tgt_fr_ctx\"], as_targets=True).input_tokens[0]\n",
    "    offset = len(tgt_fr_ctx_tokens) - 1 if has_target_context else 0 # pad\n",
    "    if tgt_fr_ctx_tokens[1] == \"fr_XX\" and has_target_context:\n",
    "        offset -= 1\n",
    "    curr_len = len(model.encode(ex[\"tgt_fr\"], as_targets=True).input_tokens[0]) - 1 # pad\n",
    "    out = model.attribute(\n",
    "        full_src,\n",
    "        full_tgt,\n",
    "        attribute_target=True,\n",
    "        show_progress=False,\n",
    "        attr_pos_start=offset if has_target_context else None,\n",
    "        attributed_fn=\"contrast_prob_diff\",\n",
    "        contrast_sources=ex[\"src_en\"],\n",
    "        contrast_targets=ex[\"tgt_fr\"].strip(),\n",
    "        contrast_targets_alignments=[\n",
    "            (idx_full, idx_curr) \n",
    "            for idx_curr, idx_full in enumerate(range(offset, offset + curr_len), start=1 if has_target_context else 0)\n",
    "        ],\n",
    "    )\n",
    "    has_lang_tag = out[0].source[0].token == \"en_XX\" and out[0].target[0].token == \"fr_XX\"\n",
    "    aggr_args = {}\n",
    "    src_brk_idx = [t.token for t in out[0].source].index(\"<brk>\")\n",
    "    lang_tag_offset = 1 if has_lang_tag else 0\n",
    "    aggr_args[\"source_spans\"] = [(lang_tag_offset,src_brk_idx), (src_brk_idx+1,len(out[0].source) - 1)]\n",
    "    if has_target_context:\n",
    "        special_tok = 'fr_XX → <brk>' if has_lang_tag else '<brk>' \n",
    "        tgt_brk_idx = [t.token for t in out[0].target].index(special_tok)\n",
    "        aggr_args[\"target_spans\"] = [(lang_tag_offset,tgt_brk_idx)]\n",
    "    aggr_out = out.aggregate(\"spans\", **aggr_args).aggregate()\n",
    "    assert aggr_out[0].source_attributions.size(0) == 4 + lang_tag_offset, (\n",
    "        f\"Expected {4 + lang_tag_offset} source tokens but found {aggr_out[0].source_attributions.size(0)} \"\n",
    "        f\"instead: {aggr_out[0].source}\"\n",
    "    )\n",
    "    if has_lang_tag:\n",
    "        aggr_out[0].step_scores[\"src_langtag_attr\"] = aggr_out[0].source_attributions[0, :]\n",
    "    aggr_out[0].step_scores[\"src_ctx_attr\"] = aggr_out[0].source_attributions[0 + lang_tag_offset, :]\n",
    "    aggr_out[0].step_scores[\"src_brk_attr\"] = aggr_out[0].source_attributions[1 + lang_tag_offset, :]\n",
    "    aggr_out[0].step_scores[\"src_curr_attr\"] = aggr_out[0].source_attributions[2 + lang_tag_offset, :]\n",
    "    aggr_out[0].step_scores[\"src_eos_attr\"] = aggr_out[0].source_attributions[3 + lang_tag_offset, :]\n",
    "    tgt_curr_start_idx = 0\n",
    "    if has_lang_tag:\n",
    "        aggr_out[0].step_scores[\"tgt_langtag_attr\"] = aggr_out[0].target_attributions[tgt_curr_start_idx, :]\n",
    "        tgt_curr_start_idx += 1\n",
    "    if has_target_context:\n",
    "        aggr_out[0].step_scores[\"tgt_ctx_attr\"] = aggr_out[0].target_attributions[tgt_curr_start_idx, :]\n",
    "        aggr_out[0].step_scores[\"tgt_brk_attr\"] = aggr_out[0].target_attributions[tgt_curr_start_idx + 1, :]\n",
    "        tgt_curr_start_idx += 2\n",
    "    aggr_out[0].step_scores[\"tgt_curr_attr\"] = aggr_out[0].target_attributions[tgt_curr_start_idx:, :].nansum(axis=0)\n",
    "    assert torch.allclose(torch.stack(list(aggr_out[0].step_scores.values()), dim=1).nansum(axis=1), torch.ones_like(aggr_out[0].step_scores[\"src_ctx_attr\"]))\n",
    "    df = pd.DataFrame(aggr_out.get_scores_dicts(do_aggregation=False)[0][\"step_scores\"])\n",
    "    df = df.transpose().reset_index().rename(columns={\"level_0\": \"token_idx\", \"level_1\": \"token\"})\n",
    "    if has_target_context:\n",
    "        df[\"token_idx\"] = [i for i in range(len(aggr_out[0].step_scores[\"src_ctx_attr\"]))]\n",
    "    df.insert(0, \"example_idx\", idx)\n",
    "    return df\n",
    "\n",
    "\n",
    "def context_sensitive_span_identification_scores(\n",
    "    examples_path: Optional[str] = None,\n",
    "    cwd: int = 1,\n",
    "    ctx: int = 4,\n",
    "    model_type: str = \"\",\n",
    "    dataset: str = \"scat\",\n",
    "    model_name: Optional[str] = None,\n",
    "    force_gen: bool = False,\n",
    "    has_context: bool = True,\n",
    "    has_lang_tag: bool = False,\n",
    "    has_target_context: bool = False,\n",
    "    start_idx: int = 0,\n",
    "    max_idx: Optional[int] = None,\n",
    "    add_tags: bool = True,\n",
    "    attribute_fn: Callable[[Dict[str, str], int], pd.DataFrame] = base_attribute_fn,\n",
    ") -> pd.DataFrame:\n",
    "    if examples_path is None:\n",
    "        model_id, model_name = get_model_id_and_name(cwd=cwd, ctx=ctx, model_type=model_type, dataset=dataset, model_name=model_name)\n",
    "        examples = get_formatted_examples(\n",
    "            model_name = model_name, force_gen = force_gen, has_context = has_context, has_lang_tag = has_lang_tag, has_target_context = has_target_context, start_idx = start_idx, max_idx = max_idx\n",
    "        )\n",
    "    else:\n",
    "        examples = pd.read_csv(examples_path, sep=\"\\t\").to_dict(\"records\")\n",
    "        model_id = model_type\n",
    "    scores_df = None\n",
    "    if start_idx > 0:\n",
    "        scores_df = pd.read_csv(f\"translations/scores/temp/{model_id}-scores-{'gold' if force_gen else 'gen'}.tsv\", sep=\"\\t\")\n",
    "    if max_idx is None:\n",
    "        max_idx = len(examples)\n",
    "    model = inseq.load_model(model_name, \"saliency\")\n",
    "    if has_lang_tag:\n",
    "        model.tokenizer.src_lang = \"en_XX\"\n",
    "        model.tokenizer.tgt_lang = \"fr_XX\"\n",
    "    for idx, ex in enumerate(examples):\n",
    "        if idx < start_idx:\n",
    "            continue\n",
    "        if idx >= max_idx:\n",
    "            break\n",
    "        df = attribute_fn(ex, model, idx)\n",
    "        if df is None:\n",
    "            continue\n",
    "        if add_tags:\n",
    "            try:\n",
    "                if force_gen:\n",
    "                    cue_tags, target_tags = get_model_cue_target_tags(ex[\"orig_fr_with_tags\"], ex[\"orig_fr\"], model)\n",
    "                else:\n",
    "                    word_tok_ctx_gen = tokenize(ex[\"fr\"])\n",
    "                    sub_tok_ctx_gen = tokenize_model(ex[\"fr\"], model)\n",
    "                    # Get cue and target tags on the gold word-tokenized text\n",
    "                    tok_gold_ref, gold_word_cue_tags, gold_word_target_tags = get_tokens_with_cue_target_tags(ex[\"orig_fr_with_tags\"], ex[\"tgt_fr\"])\n",
    "                    # Align the word-tokenized model generation to the word-tokenized gold text\n",
    "                    ctx_gen_to_gold_ref_alignments = aligner.get_word_aligns(word_tok_ctx_gen, tok_gold_ref)[\"itermax\"]\n",
    "                    # Tags on the model-generated word level translation\n",
    "                    ctx_gen_word_cue_tags = propagate_tags(word_tok_ctx_gen, gold_word_cue_tags, ctx_gen_to_gold_ref_alignments)\n",
    "                    ctx_gen_word_target_tags = propagate_tags(word_tok_ctx_gen, gold_word_target_tags, ctx_gen_to_gold_ref_alignments)\n",
    "                    # Align the subword- and word-tokenized model generations\n",
    "                    try:\n",
    "                        sub_to_word_ctx_gen_alignments = get_subword_alignments(\" \".join(sub_tok_ctx_gen), \" \".join(word_tok_ctx_gen))\n",
    "                    except AssertionError:\n",
    "                        raise ValueError(sub_tok_ctx_gen, word_tok_ctx_gen)\n",
    "                    # Propagate word-level tags on model generation to subword level.\n",
    "                    cue_tags = propagate_tags(sub_tok_ctx_gen, ctx_gen_word_cue_tags, sub_to_word_ctx_gen_alignments)\n",
    "                    target_tags = propagate_tags(sub_tok_ctx_gen, ctx_gen_word_target_tags, sub_to_word_ctx_gen_alignments)\n",
    "                    if idx < 3:\n",
    "                        print([(x, y) for x, y in zip(sub_tok_ctx_gen, cue_tags)])\n",
    "                        print([(x, y) for x, y in zip(sub_tok_ctx_gen, target_tags)])\n",
    "                # Add </s> token tag\n",
    "                cue_tags += [0]\n",
    "                target_tags += [0]\n",
    "                if has_lang_tag:\n",
    "                    cue_tags = [0] + cue_tags\n",
    "                    target_tags = [0] + target_tags\n",
    "                df[\"is_supporting_context\"] = cue_tags\n",
    "                df[\"is_context_sensitive\"] = target_tags\n",
    "            except Exception as ex:\n",
    "                print(f\"Excluding example {idx} due to error {ex}\")\n",
    "                continue\n",
    "        if scores_df is None:\n",
    "            scores_df = df\n",
    "        else:\n",
    "            scores_df = pd.concat([scores_df, df], axis=0)\n",
    "        scores_df.to_csv(f\"translations/scores/temp/{model_id}-scores-{'gold' if force_gen else 'gen'}.tsv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = \"context-mt/scat-mbart50-1toM-target-ctx4-cwd0-en-fr\"\n",
    "#examples = get_formatted_examples(\n",
    "#    model_name = idx,\n",
    "#    has_context=True,\n",
    "#    has_target_context=True,\n",
    "#    has_lang_tag=True,\n",
    "#    force_gen=False,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(examples)\n",
    "#df.to_csv(f\"translations/processed_examples/{idx.split('/')[1]}.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sensitive_span_identification_scores(\n",
    "    model_name=\"context-mt/scat-marian-big-ctx4-cwd1-en-fr\",\n",
    "    model_type=\"scat-marian-big-ctx4-cwd1\",\n",
    "    examples_path=\"translations/processed_examples/scat-marian-big-ctx4-cwd1-en-fr.tsv\",\n",
    "    attribute_fn=input_contributions_attribute_fn,\n",
    "    has_context=True,\n",
    "    has_target_context=False,\n",
    "    has_lang_tag=False,\n",
    "    force_gen=False,\n",
    "    add_tags=False, #! important\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#\n",
    "#df = pd.read_csv(\"translations/scores/scat-marian-big-target-ctx4-cwd0-scores-gen.tsv\", sep=\"\\t\")\n",
    "#cols = [\"src_ctx_attr\", \"src_brk_attr\", \"src_curr_attr\", \"src_eos_attr\", \"tgt_ctx_attr\", \"tgt_brk_attr\", \"tgt_curr_attr\"]\n",
    "#df = df[[x for x in df.columns if x not in cols]]\n",
    "#df.to_csv(\"translations/scores/scat-marian-big-target-ctx4-cwd0-scores-gen.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sensitive_span_identification_scores(\n",
    "    model_name=\"context-mt/scat-marian-big-target-ctx4-cwd0-en-fr\",\n",
    "    model_type=\"scat-marian-big-target-ctx4-cwd0\",\n",
    "    examples_path=\"translations/processed_examples/scat-marian-big-target-ctx4-cwd0-en-fr.tsv\",\n",
    "    attribute_fn=input_contributions_attribute_fn,\n",
    "    has_context=True,\n",
    "    has_target_context=True,\n",
    "    has_lang_tag=False,\n",
    "    force_gen=False,\n",
    "    add_tags=False, #! important\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sensitive_span_identification_scores(\n",
    "    model_name=\"context-mt/scat-marian-small-ctx4-cwd1-en-fr\",\n",
    "    model_type=\"scat-marian-small-ctx4-cwd1\",\n",
    "    examples_path=\"translations/processed_examples/scat-marian-small-ctx4-cwd1-en-fr.tsv\",\n",
    "    attribute_fn=input_contributions_attribute_fn,\n",
    "    has_context=True,\n",
    "    has_target_context=False,\n",
    "    has_lang_tag=False,\n",
    "    force_gen=False,\n",
    "    add_tags=False, #! important\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sensitive_span_identification_scores(\n",
    "    model_name=\"context-mt/scat-marian-small-target-ctx4-cwd0-en-fr\",\n",
    "    model_type=\"scat-marian-small-target-ctx4-cwd0\",\n",
    "    examples_path=\"translations/processed_examples/scat-marian-small-target-ctx4-cwd0-en-fr.tsv\",\n",
    "    attribute_fn=input_contributions_attribute_fn,\n",
    "    has_context=True,\n",
    "    has_target_context=True,\n",
    "    has_lang_tag=False,\n",
    "    force_gen=False,\n",
    "    add_tags=False, #! important\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sensitive_span_identification_scores(\n",
    "    model_name=\"context-mt/scat-mbart50-1toM-ctx4-cwd1-en-fr\",\n",
    "    model_type=\"scat-mbart50-1toM-ctx4-cwd1\",\n",
    "    examples_path=\"translations/processed_examples/scat-mbart50-1toM-ctx4-cwd1-en-fr.tsv\",\n",
    "    attribute_fn=input_contributions_attribute_fn,\n",
    "    has_context=True,\n",
    "    has_target_context=False,\n",
    "    has_lang_tag=True,\n",
    "    force_gen=False,\n",
    "    add_tags=False, #! important\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping example 319\n",
      "Skipping example 369\n",
      "Skipping example 389\n",
      "Skipping example 423\n",
      "Skipping example 444\n",
      "Skipping example 461\n",
      "Skipping example 499\n"
     ]
    }
   ],
   "source": [
    "context_sensitive_span_identification_scores(\n",
    "    model_name=\"context-mt/scat-mbart50-1toM-target-ctx4-cwd0-en-fr\",\n",
    "    model_type=\"scat-mbart50-1toM-target-ctx4-cwd0\",\n",
    "    examples_path=\"translations/processed_examples/scat-mbart50-1toM-target-ctx4-cwd0-en-fr.tsv\",\n",
    "    attribute_fn=input_contributions_attribute_fn,\n",
    "    has_context=True,\n",
    "    has_target_context=True,\n",
    "    has_lang_tag=True,\n",
    "    force_gen=False,\n",
    "    add_tags=False, #! important\n",
    "    start_idx=319,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sensitive_span_identification_scores(\n",
    "    cwd=1, ctx=4,\n",
    "    model_type=\"scat-marian-big-ctx4-cwd1\",\n",
    "    model_name=\"context-mt/scat-marian-big-ctx4-cwd1-en-fr\",\n",
    "    has_context=True,\n",
    "    has_target_context=False,\n",
    "    has_lang_tag=False,\n",
    "    force_gen=False,\n",
    "    add_tags=False, #! important\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sensitive_span_identification_scores(\n",
    "    cwd=0, ctx=4,\n",
    "    model_type=\"scat-marian-big-target-ctx4-cwd0\",\n",
    "    model_name=\"context-mt/scat-marian-big-target-ctx4-cwd0-en-fr\",\n",
    "    has_context=True,\n",
    "    has_target_context=True,\n",
    "    has_lang_tag=False,\n",
    "    force_gen=False,\n",
    "    add_tags=False, #! important\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sensitive_span_identification_scores(\n",
    "    cwd=0, ctx=4,\n",
    "    model_type=\"scat-marian-small-target-ctx4-cwd0\",\n",
    "    model_name=\"context-mt/scat-marian-small-target-ctx4-cwd0-en-fr\",\n",
    "    has_context=True,\n",
    "    has_target_context=True,\n",
    "    has_lang_tag=False,\n",
    "    force_gen=False,\n",
    "    add_tags=False, #! important\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sensitive_span_identification_scores(\n",
    "    cwd=1, ctx=4,\n",
    "    model_type=\"scat-marian-small-ctx4-cwd1\",\n",
    "    model_name=\"context-mt/scat-marian-small-ctx4-cwd1-en-fr\",\n",
    "    has_context=True,\n",
    "    has_target_context=False,\n",
    "    has_lang_tag=False,\n",
    "    force_gen=False,\n",
    "    add_tags=False, #! important\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sensitive_span_identification_scores(\n",
    "    cwd=0, ctx=4,\n",
    "    model_type=\"scat-mbart50-1toM-target-ctx4-cwd0\",\n",
    "    model_name=\"context-mt/scat-mbart50-1toM-target-ctx4-cwd0-en-fr\",\n",
    "    has_context=True,\n",
    "    has_target_context=True,\n",
    "    has_lang_tag=True,\n",
    "    force_gen=False,\n",
    "    add_tags=False, #! important\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sensitive_span_identification_scores(\n",
    "    cwd=1, ctx=4,\n",
    "    model_type=\"scat-mbart50-1toM-ctx4-cwd1\",\n",
    "    model_name=\"context-mt/scat-mbart50-1toM-ctx4-cwd1-en-fr\",\n",
    "    has_context=True,\n",
    "    has_target_context=False,\n",
    "    has_lang_tag=True,\n",
    "    force_gen=False,\n",
    "    add_tags=False, #! important\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type, model_name in default_models.items():\n",
    "    if model_type == \"mbart50-1toM\":\n",
    "        context_sensitive_span_identification_scores(\n",
    "            cwd=0, ctx=0,\n",
    "            model_type=model_type,\n",
    "            model_name=model_name,\n",
    "            has_context=False,\n",
    "            has_target_context=False,\n",
    "            has_lang_tag=model_type == \"mbart50-1toM\",\n",
    "            force_gen=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "def load_scores_df(path: str):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    df[\"source_context\"] = [scat[\"context_en\"][idx] for idx in df[\"example_idx\"]]\n",
    "    df[\"source\"] = [scat[\"en\"][idx] for idx in df[\"example_idx\"]]\n",
    "    df[\"target_context\"] = [scat[\"context_fr\"][idx] for idx in df[\"example_idx\"]]\n",
    "    df[\"target\"] = df.groupby('example_idx')['token'].transform(lambda x: ''.join(x.str.replace(\"▁\", \" \")))\n",
    "    df[\"kl_divergence_mean\"] = df.groupby('example_idx')['kl_divergence'].transform(np.mean)\n",
    "    df[\"kl_divergence_std\"] = df.groupby('example_idx')['kl_divergence'].transform(np.std)\n",
    "    df[\"kl_divergence_zscore\"] = (df.kl_divergence - df.kl_divergence_mean) / df.kl_divergence_std\n",
    "    df[\"pcxmi_mean\"] = df.groupby('example_idx')['pcxmi'].transform(np.mean)\n",
    "    df[\"pcxmi_std\"] = df.groupby('example_idx')['pcxmi'].transform(np.std)\n",
    "    df[\"pcxmi_zscore\"] = (df.pcxmi - df.pcxmi_mean) / df.pcxmi_std\n",
    "    df[\"contrast_prob_diff\"]  = df.contrast_prob - df.probability\n",
    "    df.is_context_sensitive = df.is_context_sensitive.astype(bool)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold = load_scores_df(\"translations/scores/scat-marian-big-ctx4-cwd1-scores-gold.tsv\")\n",
    "print(df_gold[\"is_context_sensitive\"].value_counts())\n",
    "df_gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_gold, x=df_gold.index, y='kl_divergence', color='is_context_sensitive', hover_data=['token', 'source_context', \"source\", \"target_context\", \"target\"], trendline=\"ols\")\n",
    "fig.write_html(\"translations/scores/scat-marian-big-ctx4-cwd1-scores-kldiv.html\")\n",
    "fig = px.scatter(df_gold, x=df_gold.index, y='kl_divergence_zscore', color='is_context_sensitive', hover_data=['token', 'source_context', \"source\", \"target_context\", \"target\"], trendline=\"ols\")\n",
    "fig.write_html(\"translations/scores/scat-marian-big-ctx4-cwd1-scores-kldiv-zscore.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_gold, x=df_gold.index, y='pcxmi', color='is_context_sensitive', hover_data=['token', 'source_context', \"source\", \"target_context\", \"target\"], trendline=\"ols\")\n",
    "fig.write_html(\"translations/scores/scat-marian-big-ctx4-cwd1-scores-pcxmi.html\")\n",
    "fig = px.scatter(df_gold, x=df_gold.index, y='pcxmi_zscore', color='is_context_sensitive', hover_data=['token', 'source_context', \"source\", \"target_context\", \"target\"], trendline=\"ols\")\n",
    "fig.write_html(\"translations/scores/scat-marian-big-ctx4-cwd1-scores-pcxmi-zscore.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen = load_scores_df(\"translations/scores/scat-marian-big-ctx4-cwd1-scores-gen.tsv\")\n",
    "#df_gen = load_scores_df(\"translations/scores/mbart50-1toM-scores-gen.tsv\")\n",
    "print(df_gen[\"is_context_sensitive\"].value_counts())\n",
    "df_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen_inter = df_gen[df_gen.example_idx < 250]\n",
    "df_gen_intra = df_gen[df_gen.example_idx >= 250]\n",
    "df_gen_inter.is_supporting_context.sum(), df_gen_intra.is_supporting_context.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "df_gen_sensitive = df_gen[df_gen[\"is_context_sensitive\"] == True]\n",
    "df_gen_not_sensitive = df_gen[df_gen[\"is_context_sensitive\"] == False]\n",
    "rng = np.random.default_rng()\n",
    "stats.kstest(df_gen_sensitive[\"kl_divergence\"].values, df_gen_not_sensitive[\"kl_divergence\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = px.scatter(df_gold, x=df_gold.index, y='kl_divergence', color='is_context_sensitive', hover_data=['token', 'source_context', \"source\", \"target_context\", \"target\"], trendline=\"ols\")\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_gen_inter, x=df_gen_inter.index, y='kl_divergence', color='is_context_sensitive', hover_data=['token', 'source_context', \"source\", \"target_context\", \"target\"], trendline=\"ols\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html(\"translations/scores/mbart50-1toM-scores-gen.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_gen, x=df_gen.index, y='pcxmi_zscore', color='is_context_sensitive', hover_data=['token', 'source_context', \"source\", \"target_context\", \"target\"], trendline=\"ols\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\"\n",
    "\n",
    "with open(f\"filtered_scat/scat/highlighted.{split}.context.en\") as f:\n",
    "    orig_ctx_en = f.readlines()\n",
    "with open(f\"filtered_scat/scat/highlighted.{split}.context.fr\") as f:\n",
    "    orig_ctx_fr = f.readlines()\n",
    "with open(f\"filtered_scat/scat/highlighted.{split}.en\") as f:\n",
    "    orig_tgt_en = f.readlines()\n",
    "with open(f\"filtered_scat/scat/highlighted.{split}.fr\") as f:\n",
    "    orig_tgt_fr = f.readlines()\n",
    "\n",
    "with open(f\"highlighted.{split}.full\", \"w\") as f:\n",
    "    for ctx_en, ctx_fr, tgt_en, tgt_fr in zip(orig_ctx_en, orig_ctx_fr, orig_tgt_en, orig_tgt_fr):\n",
    "        f.write(f\"<SRC_CTX> {ctx_en.strip()} <SRC> {tgt_en.strip()} <TGT_CTX> {ctx_fr.strip()} <TGT> {tgt_fr.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "with open(f\"highlighted.test.full\") as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    t for t in texts \n",
    "    if t.split(\"<TGT>\")[1].count(\"<hon>\") == 1 and t.split(\"<TGT>\")[1].count(\"<hoff>\") == 1 and t.split(\"<TGT>\")[1].count(\"<p>\") == 1 and t.split(\"<TGT>\")[1].count(\"</p>\") == 1\n",
    "    and t.split(\"<SRC>\")[1].split(\"<TGT_CTX>\")[0].count(\"<p>\") == 1 and t.split(\"<SRC>\")[1].split(\"<TGT_CTX>\")[0].count(\"</p>\") == 1\n",
    "    and \"<hon>\" not in t.split(\"<TGT>\")[0] and t.count(\"<hon>\") == 1 and t.count(\"<hoff>\") == 1\n",
    "]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(txt: str):\n",
    "    return \"P\" if \"<p>\" in txt else \"H\" if \"<hon>\" in txt else \"O\"\n",
    "\n",
    "def find_replace_tags(text: str):\n",
    "    matches = re.finditer(r'(?:<p>|<hon>)([^<]+)(?:</p>|<hoff>)', text)\n",
    "    tags = [(m.group(1), text[m.start():m.end()], m.start(), m.end()) for m in matches]\n",
    "    new_tags = []\n",
    "    for idx in range(len(tags)):\n",
    "        diff_len = len(tags[idx][1]) - len(tags[idx][0])\n",
    "        text = text.replace(tags[idx][1], tags[idx][0], 1)\n",
    "        new_end = tags[idx][3] - diff_len\n",
    "        if new_end - tags[idx][2] > 1 and tags[idx][0].strip() and new_end > tags[idx][2]:\n",
    "            new_tags.append({\"start\": tags[idx][2], \"end\": new_end, \"label\": get_label(tags[idx][1])})\n",
    "        for idx2 in range(idx + 1, len(tags)):\n",
    "            tags[idx2] = (tags[idx2][0], tags[idx2][1], tags[idx2][2] - diff_len, tags[idx2][3] - diff_len)\n",
    "    return text, new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "pattern_nonspace = r\"(<p>|</p>|<hon>|<hoff>|<SRC_CTX>|<SRC>|<TGT_CTX>|<TGT>|\\S+)\"\n",
    "pattern_word = r\"(<p>|</p>|<hon>|<hoff>|<SRC_CTX>|<SRC>|<TGT_CTX>|<TGT>|\\w+)\"\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return [x for nonspace in re.split(pattern_nonspace, text) for x in re.split(pattern_word, nonspace) if x.strip() and x not in [\"<p>\", \"</p>\", \"<hon>\", \"<hoff>\"]]\n",
    "\n",
    "tokens = [tokenize(text) for text in texts]\n",
    "texts, tags = zip(*[find_replace_tags(text) for text in texts])\n",
    "df = pd.DataFrame({\"text\": texts, \"prediction\": tags, \"tokens\": tokens})\n",
    "df.to_json(\"scat_argilla_target.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load dataset from the hub\n",
    "dataset = load_dataset(\"private-demos/scat_argilla_target\", split=\"train\", download_mode=\"force_redownload\")\n",
    "\n",
    "rg.init(api_url=\"https://private-demos-argilla-test.hf.space\", api_key=\"admin.apikey\")\n",
    "\n",
    "# read in dataset, assuming its a dataset for token classification\n",
    "dataset_rg = rg.read_datasets(dataset, task=\"TokenClassification\")\n",
    "\n",
    "# log the dataset\n",
    "rg.log(dataset_rg, \"scat_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "rg.init(api_url=\"https://private-demos-argilla-test.hf.space\", api_key=\"admin.apikey\")\n",
    "df = rg.load(\"scat_target\").to_datasets().to_pandas()\n",
    "df = df[df[\"status\"] == \"Validated\"]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_texts = []\n",
    "for idx, row in df.iterrows():\n",
    "    annotations = sorted([(dic[\"start\"], dic[\"end\"], dic[\"label\"]) for dic in row[\"annotation\"]], key=lambda x: x[0])\n",
    "    text = row[\"text\"]\n",
    "    for idx in range(len(annotations)):\n",
    "        add = 0\n",
    "        if annotations[idx][2] == \"P\":\n",
    "            text = text[:annotations[idx][0]] + \"<p>\" + text[annotations[idx][0]:annotations[idx][1]] + \"</p>\" + text[annotations[idx][1]:]\n",
    "            add = 7\n",
    "        elif annotations[idx][2] == \"H\":\n",
    "            text = text[:annotations[idx][0]] + \"<hon>\" + text[annotations[idx][0]:annotations[idx][1]] + \"<hoff>\" + text[annotations[idx][1]:]\n",
    "            add = 11\n",
    "        for idx2 in range(idx + 1, len(annotations)):\n",
    "            annotations[idx2] = (annotations[idx2][0] + add, annotations[idx2][1] + add, annotations[idx2][2])\n",
    "    tagged_texts.append(text)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_en = [t.split(\"<SRC_CTX>\")[1].split(\"<SRC>\")[0].strip() for t in tagged_texts]\n",
    "tgt_en = [t.split(\"<SRC>\")[1].split(\"<TGT_CTX>\")[0].strip() for t in tagged_texts]\n",
    "ctx_fr = [t.split(\"<TGT_CTX>\")[1].split(\"<TGT>\")[0].strip() for t in tagged_texts]\n",
    "tgt_fr = [t.split(\"<TGT>\")[1].strip() for t in tagged_texts]\n",
    "len(ctx_en), len(tgt_en), len(ctx_fr), len(tgt_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"filtered.test.context.en\", \"a+\") as f:\n",
    "    for line in ctx_en:\n",
    "        f.write(line + \"\\n\")\n",
    "with open(\"filtered.test.en\", \"a+\") as f:\n",
    "    for line in tgt_en:\n",
    "        f.write(line + \"\\n\")\n",
    "with open(\"filtered.test.context.fr\", \"a+\") as f:\n",
    "    for line in ctx_fr:\n",
    "        f.write(line + \"\\n\")\n",
    "with open(\"filtered.test.fr\", \"a+\") as f:\n",
    "    for line in tgt_fr:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    "- Add columns `is_context_sensitive` and `is_supporting_context` to scores dataframes. First, map tags to tokens tokenized using `re.split` on word boundaries. Then use code from `peviz` to remap scores to subword tokens following model tokenization. `</s>` is assigned an extra 0 at the end by default.\n",
    "\n",
    "- Add `likelihood_ratio` metric to Inseq and allow `top_p` for `kl_divergence` metric. Add `kl_div_0.90` and `likelihood_ratio` columns to scores dataframes.\n",
    "\n",
    "- Plot scores distribution for different metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCAT Issues:\n",
    "- Malformed tags (easy to fix)\n",
    "- It impersonal vs it ambiguous (hard to fix automatically)\n",
    "- Wrong annotations, bias on context (cannot be fixed automatically)\n",
    "\n",
    "Method's issues:\n",
    "- Using gold reference as target for comparison? Generate tgt, align with gold, measure metric on tgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inseq\n",
    "\n",
    "model = inseq.load_model(\"context-mt/scat-marian-big-target-ctx4-cwd0-en-fr\", \"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = scat[11]\n",
    "src = ex[\"context_en\"] + \"<brk> \" + ex[\"en\"]\n",
    "ctx_gen = model.generate(src, max_new_tokens=512, skip_special_tokens=False)[0].split(\"<brk>\")[1].strip().strip(\"</s>\")\n",
    "noctx_gen = \"S'ils le font, ils ne se vantent pas aussi fort.\" #model.generate(ex[\"en\"], max_new_tokens=512)[0]\n",
    "print(ctx_gen)\n",
    "print(noctx_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.encode(ctx_gen, as_targets=True).input_tokens[0])\n",
    "print(model.encode(noctx_gen, as_targets=True).input_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tgt = model.attribute(\n",
    "    src,\n",
    "    ctx_gen,\n",
    "    method=\"input_x_gradient\",\n",
    "    attribute_target=True,\n",
    "    attributed_fn=\"contrast_prob_diff\",\n",
    "    step_scores=[\"contrast_prob_diff\"],\n",
    "    contrast_targets=noctx_gen,\n",
    "    contrast_targets_alignments=\"auto\"\n",
    "    \n",
    ")\n",
    "out_tgt.weight_attributions(\"contrast_prob_diff\")\n",
    "out_tgt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inseq\n",
    "model = inseq.load_model(\"context-mt/scat-marian-big-ctx4-cwd1-en-fr\", \"saliency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"Yes. And an egg, but only once a week. How's that? Good. Where did these eggs come from?<brk> Are they the hotel's?\"\n",
    "tgt = \"Oui. Et un oeuf, mais seulement une fois par semaine. Comment va-t-il? Bon. D'où proviennent ces oeuf?<brk> Ils sont à l'hôtel?\"\n",
    "\n",
    "has_pad = True\n",
    "\n",
    "tgt_prefix, tgt_curr = tgt.split(\"<brk>\")\n",
    "tgt_prefix = tgt_prefix.strip() + \"<brk>\"\n",
    "tgt_curr = tgt_curr.strip()\n",
    "\n",
    "src = \"Okay, okay. Calm down. First of all, you did the right thing by hiding under this table. Secondly, your man is here. I'm gonna take care of this for us. I've been playing Xbox for years.<brk> I'm really good at fixing it when it freezes.\"\n",
    "#tgt_prefix = \"D'accord, d'accord. Calme-toi. D'abord, tu as fait ce qu'il fallait en te cachant sous cette table. Deuxièmement, ton homme est là. Je vais m'en occuper pour nous. Je joue à la Xbox depuis des années.<brk>\"\n",
    "tgt_curr = \"Je suis vraiment doué pour la réparer quand elle gèle.\"\n",
    "tgt = tgt_curr #tgt_prefix + \" \" + tgt_curr\n",
    "\n",
    "src_curr = src.split(\"<brk>\")[1].strip()\n",
    "offset = 0 #len(model.encode(tgt_prefix, as_targets=True).input_tokens[0])\n",
    "curr_len = len(model.encode(tgt_curr, as_targets=True).input_tokens[0])\n",
    "\n",
    "if has_pad:\n",
    "#    offset -= 1\n",
    "    curr_len -= 1\n",
    "\n",
    "out = model.attribute(\n",
    "    src,\n",
    "    tgt,\n",
    "    attribute_target=True,\n",
    "    show_progress=False,\n",
    "    #attr_pos_start=offset,\n",
    "    step_scores=[\"probability\", \"contrast_prob\", \"contrast_prob_diff\"],\n",
    "    attributed_fn=\"contrast_prob_diff\",\n",
    "    contrast_sources=src_curr,\n",
    "    contrast_targets=tgt_curr,\n",
    "    contrast_targets_alignments=[(idx_full, idx_curr) for idx_curr, idx_full in enumerate(range(offset, offset + curr_len), start=0)],\n",
    ")\n",
    "out.show(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "def contributions_attribute_fn(ex: Dict[str, str], idx: int) -> pd.DataFrame:\n",
    "    has_target_context = ex[\"tgt_fr_ctx\"] is not None\n",
    "    full_src = ex[\"src_en_ctx\"] + \" \" + ex[\"src_en\"]\n",
    "    full_tgt = ex[\"tgt_fr_ctx\"] + \" \" + ex[\"tgt_fr\"] if ex[\"tgt_fr_ctx\"] else ex[\"tgt_fr\"]\n",
    "    offset = len(model.encode(ex[\"tgt_fr_ctx\"], as_targets=True).input_tokens[0]) - 1 if has_target_context else 0 # pad\n",
    "    curr_len = len(model.encode(ex[\"tgt_fr\"], as_targets=True).input_tokens[0]) - 1 # pad\n",
    "    out = model.attribute(\n",
    "        full_src,\n",
    "        full_tgt,\n",
    "        attribute_target=True,\n",
    "        show_progress=False,\n",
    "        attr_pos_start=offset if has_target_context else None,\n",
    "        attributed_fn=\"contrast_prob_diff\",\n",
    "        contrast_sources=ex[\"src_en\"],\n",
    "        contrast_targets=ex[\"tgt_fr\"],\n",
    "        contrast_targets_alignments=[\n",
    "            (idx_full, idx_curr) \n",
    "            for idx_curr, idx_full in enumerate(range(offset, offset + curr_len), start=1 if has_target_context else 0)\n",
    "        ],\n",
    "    )\n",
    "    has_lang_tag = out[0].source[0].token == \"en_XX\" and out[0].target[0].token == \"fr_XX\"\n",
    "    aggr_args = {}\n",
    "    src_brk_idx = [t.token for t in out[0].source].index(\"<brk>\")\n",
    "    lang_tag_offset = 1 if has_lang_tag else 0\n",
    "    aggr_args[\"source_spans\"] = [(lang_tag_offset,src_brk_idx), (src_brk_idx+1,len(out[0].source) - 1)]\n",
    "    if has_target_context:\n",
    "        tgt_brk_idx = [t.token for t in out[0].target].index(\"<brk>\")\n",
    "        aggr_args[\"target_spans\"] = [(lang_tag_offset,tgt_brk_idx)]\n",
    "    aggr_out = out.aggregate(\"spans\", **aggr_args).aggregate()\n",
    "    assert aggr_out[0].source_attributions.size(0) == 4 + lang_tag_offset, (\n",
    "        f\"Expected {4 + lang_tag_offset} source tokens but found {aggr_out[0].source_attributions.size(0)} \"\n",
    "        f\"instead: {aggr_out[0].source}\"\n",
    "    )\n",
    "    if has_lang_tag:\n",
    "        aggr_out[0].step_scores[\"src_langtag_attr\"] = aggr_out[0].source_attributions[0, :]\n",
    "    aggr_out[0].step_scores[\"src_ctx_attr\"] = aggr_out[0].source_attributions[0 + lang_tag_offset, :]\n",
    "    aggr_out[0].step_scores[\"src_brk_attr\"] = aggr_out[0].source_attributions[1 + lang_tag_offset, :]\n",
    "    aggr_out[0].step_scores[\"src_curr_attr\"] = aggr_out[0].source_attributions[2 + lang_tag_offset, :]\n",
    "    aggr_out[0].step_scores[\"src_eos_attr\"] = aggr_out[0].source_attributions[3 + lang_tag_offset, :]\n",
    "    tgt_curr_start_idx = 0\n",
    "    if has_lang_tag:\n",
    "        aggr_out[0].step_scores[\"tgt_langtag_attr\"] = aggr_out[0].target_attributions[tgt_curr_start_idx, :]\n",
    "        tgt_curr_start_idx += 1\n",
    "    if has_target_context:\n",
    "        aggr_out[0].step_scores[\"tgt_ctx_attr\"] = aggr_out[0].target_attributions[tgt_curr_start_idx, :]\n",
    "        aggr_out[0].step_scores[\"tgt_brk_attr\"] = aggr_out[0].target_attributions[tgt_curr_start_idx + 1, :]\n",
    "        tgt_curr_start_idx += 2\n",
    "    aggr_out[0].step_scores[\"tgt_curr_attr\"] = aggr_out[0].target_attributions[tgt_curr_start_idx:, :].nansum(axis=0)\n",
    "    aggr_out.show(do_aggregation=False)\n",
    "    assert torch.allclose(torch.stack(list(aggr_out[0].step_scores.values()), dim=1).nansum(axis=1), torch.ones_like(aggr_out[0].step_scores[\"src_ctx_attr\"]))\n",
    "    return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inseq\n",
    "\n",
    "#model = inseq.load_model(\"context-mt/scat-marian-big-ctx4-cwd1-en-fr\", \"saliency\")\n",
    "model = inseq.load_model(\"context-mt/scat-mbart50-1toM-ctx4-cwd1-en-fr\", \"saliency\")\n",
    "model.tokenizer.src_lang = \"en_XX\"\n",
    "model.tokenizer.tgt_lang = \"fr_XX\"\n",
    "\n",
    "ex_no_tgt_ctx = {\n",
    "    \"src_en\": \"I'm really good at fixing it when it freezes.\",\n",
    "    \"src_en_ctx\": \"Okay, okay. Calm down. First of all, you did the right thing by hiding under this table. Secondly, your man is here. I'm gonna take care of this for us. I've been playing Xbox for years.<brk>\",\n",
    "    \"tgt_fr\": \"Je suis vraiment doué pour la réparer quand elle gèle.\",\n",
    "    \"tgt_fr_ctx\": None,\n",
    "}\n",
    "\n",
    "out = contributions_attribute_fn(ex_no_tgt_ctx, 0)\n",
    "\n",
    "#model = inseq.load_model(\"context-mt/scat-marian-big-target-ctx4-cwd0-en-fr\", \"saliency\")\n",
    "\n",
    "ex_tgt_ctx = {\n",
    "    \"src_en\": \"I'm really good at fixing it when it freezes.\",\n",
    "    \"src_en_ctx\": \"Okay, okay. Calm down. First of all, you did the right thing by hiding under this table. Secondly, your man is here. I'm gonna take care of this for us. I've been playing Xbox for years.<brk>\",\n",
    "    \"tgt_fr\": \"Je suis vraiment doué pour la réparer quand elle gèle.\",\n",
    "    \"tgt_fr_ctx\": \"D'accord, d'accord. Calme-toi. D'abord, tu as fait ce qu'il fallait en te cachant sous cette table. Deuxièmement, ton homme est là. Je vais m'en occuper pour nous. Je joue à la Xbox depuis des années.<brk>\"\n",
    "}\n",
    "\n",
    "#out = contributions_attribute_fn(ex_tgt_ctx, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an is_correct column to the scores file, marking examples where the model correctly disambiguates the gender in the target sentence\n",
    "# Use to analyze data folds and not as feature, since it is not available at test time\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"scat-mbart50-1toM-target-ctx4-cwd0\"\n",
    "examples = pd.read_csv(f\"translations/processed_examples/{model_name}-en-fr.tsv\", sep=\"\\t\")\n",
    "is_correct = []\n",
    "for ref, contrast, mt in zip(scat[\"fr\"], scat[\"contrast_fr\"], examples[\"tgt_fr\"]):\n",
    "    out_scores = get_aligned_gender_annotations(ref, contrast, mt)\n",
    "    if sum(out_scores) > 0:\n",
    "        is_correct += [1]\n",
    "    else:\n",
    "        is_correct += [0]\n",
    "correct_df = pd.DataFrame({\"example_idx\": [i for i in range(len(is_correct))], \"is_correct\": is_correct})\n",
    "df = pd.read_csv(f\"translations/scores/{model_name}-scores-gen.tsv\", sep=\"\\t\")\n",
    "if \"is_correct\" in df.columns:\n",
    "    df = df.drop(\"is_correct\", axis=1)\n",
    "df = df.merge(correct_df, on=\"example_idx\", how=\"left\")\n",
    "df.to_csv(f\"translations/scores/temp/{model_name}-scores-gen.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
