import argparse
import logging
import warnings
from typing import Any, Dict, List, Optional, Tuple, Union

import torch
from rich.console import Console

import inseq
from inseq import AttributionModel
from inseq.data import FeatureAttributionOutput, FeatureAttributionSequenceOutput
from pecore.alignment_utils import tokenize_subwords
from pecore.data_utils import PECoReExample
from pecore.enums import CTIMetricsEnum, ModelTypeEnum
from pecore.inseq_utils import prepare_cci_params
from pecore.model_utils import get_lang_from_model_type, has_lang_tag

logging.basicConfig(
    format="%(asctime)s %(levelname)s %(name)s  %(message)s",
    datefmt="%d-%m-%y %H:%M:%S",
    level=logging.INFO,
)

logger = logging.getLogger(__name__)
inseq.data.aggregator.logger.setLevel(logging.WARNING)
inseq.utils.alignment_utils.logger.setLevel(logging.ERROR)
warnings.filterwarnings("ignore")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model_name",
        type=str,
        required=True,
        help="HF Hub identifier of the model used for PECoRe.",
    )
    parser.add_argument("--input", type=str, required=True, help="Input text provided to the model for generation.")
    parser.add_argument("--output", type=str, default=None, help="Output text generated by the model.")
    parser.add_argument(
        "--contextless_output",
        type=str,
        default=None,
        help="If specified, use as contextless output for contrastive attribution. Do not use with --impute_with_contextless_output.",
    )
    parser.add_argument(
        "--input_lang",
        type=str,
        default=None,
        help="Input language, used for multilingual models with language prefixes.",
    )
    parser.add_argument(
        "--output_lang",
        type=str,
        default=None,
        help="Output language, used for multilingual models with language prefixes.",
    )
    parser.add_argument(
        "--model_type",
        type=str,
        default=None,
        choices=list(ModelTypeEnum),
        help="Model type from the PECoRe model configuration, used to map languages to tags for multilingual models.",
    )
    parser.add_argument(
        "--cti_metric",
        type=str,
        default=CTIMetricsEnum.KL_DIVERGENCE,
        choices=[CTIMetricsEnum.PCXMI, CTIMetricsEnum.KL_DIVERGENCE],
        help="Metric to use for the context-sensitive target identification step.",
    )
    parser.add_argument(
        "--attribution_method",
        type=str,
        default="saliency",
        choices=inseq.list_feature_attribution_methods(),
        help="Attribution method to use for the contextual cues imputation step.",
    )
    parser.add_argument(
        "--attributed_fn",
        type=str,
        default="kl_divergence",
        choices=inseq.list_step_functions(),
        help="Step function to use as attribution target for the contextual cues imputation step.",
    )
    parser.add_argument(
        "--attributions_aggregate_fns",
        type=str,
        nargs="+",
        default=None,
        help="Aggregators to use for attribution aggregation. If not specified, method-specific defaults are used.",
    )
    parser.add_argument(
        "--normalize_attributions", action="store_true", help="If specified, attributions are normalized to sum to 1."
    )
    parser.add_argument(
        "--select_attributions_idx",
        type=int,
        nargs="+",
        default=None,
        help="Indices to select from produced attributions. Default is to use all elements.",
    )
    parser.add_argument(
        "--ctx_break",
        type=str,
        default="<brk> ",
        help="String used to mark the separation between context and current segments in the provided s",
    )
    parser.add_argument(
        "--model_use_ctx_break",
        action="store_true",
        help=(
            "If specified, the context-separation string is preserved in texts provided to the model. "
            "Use for context-aware models marking context with tags (e.g. <brk>)."
        ),
    )
    parser.add_argument(
        "--cti_scores_std_threshold",
        type=float,
        default=1,
        help="Parameter to control the standard deviation threshold to use to select context-sensitive tokens. ",
    )
    parser.add_argument(
        "--cti_scores_top_k",
        type=int,
        default=None,
        help="Parameter to select only the top K elements from the available context-sensitive target tokens.",
    )
    parser.add_argument(
        "--cci_scores_std_threshold",
        type=float,
        default=1,
        help="Parameter to control the standard deviation threshold to use to select contextual cue tokens. ",
    )
    parser.add_argument(
        "--cci_scores_top_k",
        type=int,
        default=None,
        help="Parameter to select only the top K elements from the available contextual cue tokens.",
    )
    parser.add_argument(
        "--impute_with_contextless_output",
        action="store_true",
        help=(
            "If specified, the greedy output produced by the model without context is used for the imputation"
            " step.Otherwise, the same output produced with context is used as contrastive alternative for the"
            " contextless version."
        ),
    )
    parser.add_argument(
        "--force_context_aware_output_prefix",
        action="store_true",
        help=(
            "If specified, for every CCI step the prefix of the current output produced with the given context is used"
            " up to the context-sensitive token as output prefix to generate the contrastive output. Useful only when"
            " impute_with_contextless_output is set to True. E.g. For context-aware current output 'Sont-elles à"
            " l'hotel?' the contextless output could be 'C'est à l'hotel?', which would make comparison less natural. "
            "With this option, assuming the context-sensitive word 'elles' in the sentence above, the output current "
            "prefix would be 'Sont-', forcing a more natural completion making use of a gendered pronoun."
        ),
    )
    parser.add_argument(
        "--show_attributions",
        action="store_true",
        help="If specified, the attributions computed for the contextual cue imputation step are shown.",
    )
    parser.add_argument(
        "--excluded_tokens", type=str, nargs="+", default=["</s>", "<pad>"], help="Tokens to exclude for the process."
    )
    parser.add_argument(
        "--special_characters", type=str, nargs="+", default=["▁"], help="Characters to replace with spaces in viz."
    )
    parser.add_argument(
        "--viz_path",
        type=str,
        default="pecore_viz.html",
        help="Path to save the visualization to. If not specified, the visualization is printed to stdout.",
    )
    parser.add_argument(
        "--contrast_force_inputs",
        action="store_true",
        help="If specified, the contrastive inputs are used for attribution, otherwise the original inputs are used.",
    )
    args = parser.parse_args()
    if args.ctx_break not in args.input:
        raise ValueError(
            f"The context-separation string '{args.ctx_break}' was not found in the provided inputs.\n"
            "Please provide an input containing the context-separation string."
        )
    return args


def visualize_procedure_details(
    args: argparse.Namespace,
    console: Console,
    input_context: str,
    input_current: str,
    output_context: str,
    output_full: str,
    has_output_context: bool,
    model_has_lang_tag: bool,
) -> None:
    cti_threshold_comment = []
    if args.cti_scores_std_threshold:
        cti_threshold_comment.append(f"std λ={args.cti_scores_std_threshold:.2f}")
    if args.cti_scores_top_k:
        cti_threshold_comment.append(f"top {args.cti_scores_top_k}")
    if len(cti_threshold_comment) > 0:
        cti_threshold_comment = ", ".join(cti_threshold_comment)
    else:
        cti_threshold_comment = "all"
    cci_threshold_comment = []
    if args.cci_scores_std_threshold:
        cci_threshold_comment.append(f"std λ={args.cci_scores_std_threshold:.2f}")
    if args.cci_scores_top_k:
        cci_threshold_comment.append(f"top {args.cci_scores_top_k}")
    if len(cci_threshold_comment) > 0:
        cci_threshold_comment = ", ".join(cci_threshold_comment)
    else:
        cci_threshold_comment = "all"
    output_context_comment = ""
    extra_params_comment = ""
    if has_output_context:
        output_context_comment = f"\n[bold]Output context:[/bold]\t{output_context}"
    if args.model_use_ctx_break:
        extra_params_comment += (
            f"\nUsing '{args.ctx_break}' to separate context and current inputs"
            f"{' and outputs' if has_output_context else ''}."
        )
    if model_has_lang_tag:
        extra_params_comment += (
            f"\nUsing language tags for model type '{args.model_type}' ({args.input_lang} -> {args.output_lang})."
        )
    console.print(
        f"Context with [bold green]contextual cues[/bold green] ({cci_threshold_comment}) followed by output"
        f" sentence\nwith [bold magenta]context-sensitive target spans[/bold magenta] ({cti_threshold_comment}):\n"
    )
    console.print(
        f"[bold]Input context:[/bold]\t{input_context}\n[bold]Input"
        f" current:[/bold]\t{input_current}{output_context_comment}\n[bold]Context-aware output:[/bold]\t{output_full}"
        f"{extra_params_comment}"
    )


def prepare_inputs_outputs(
    model: AttributionModel,
    input_txt: str,
    output: str,
    ctx_break: str,
    model_use_ctx_break: bool,
    excluded_tokens: List[str],
    gen_kwargs: Dict[str, Any] = {},
) -> Tuple[PECoReExample, bool]:
    if len(input_txt.split(ctx_break)) > 2:
        raise ValueError("Only a single context for inputs is supported at the moment.")
    input_context, input_current = input_txt.split(ctx_break)
    input_full = input_txt if model_use_ctx_break else " ".join([input_context, input_current])
    set_manual_output_ctx_break = False
    if not model_use_ctx_break and output is None:
        set_manual_output_ctx_break = True
    if output is None:
        output = model.generate(input_full, skip_special_tokens=False, **gen_kwargs)[0]
        if not model.is_encoder_decoder:
            output = output[len(input_full) :]
        for tok in excluded_tokens:
            output = output.replace(tok, "")
        output = output.strip()
    if set_manual_output_ctx_break:
        output = input(
            f"The following output was generate by the model: {output}\n"
            f"Rewrite it here by adding '{ctx_break}' wherever appropriate to mark context break: "
        )
    has_output_context = ctx_break in output
    if has_output_context:
        if len(output.split(ctx_break)) > 2:
            raise ValueError("Only a single context for outputs is supported at the moment.")
        output_context, output_current = output.split(ctx_break)
        output_full = " ".join([output_context, output_current])
        if model_use_ctx_break:
            output_context += ctx_break
            output_full = output
    else:
        output_context, output_current = None, output
        output_full = output
    return (
        PECoReExample(
            input_full=input_full,
            input_current=input_current,
            input_context=input_context,
            output_full=output_full if model.is_encoder_decoder else input_full + " " + output_full,
            output_current=output_current if model.is_encoder_decoder else input_current + " " + output_current,
            output_context=output_context,
        ),
        has_output_context,
    )


def scores_to_rank(
    all_scores: Union[torch.Tensor, List[torch.Tensor]],
    top_k: int,
    threshold: Optional[float] = None,
    tokens: Optional[List[str]] = None,
    excluded_tokens: List[str] = [],
) -> Union[Tuple[List[Tuple[int, float]], float], Tuple[List[List[Tuple[int, float]]], float]]:
    if not isinstance(all_scores, list):
        all_scores = [all_scores]
    all_idxs_and_scores = []
    all_filtered_scores = []
    for scores in all_scores:
        l_scores = scores.tolist()
        if excluded_tokens and tokens:
            t_scores = torch.tensor([s for idx, s in enumerate(l_scores) if tokens[idx] not in excluded_tokens])
        else:
            t_scores = scores
        all_filtered_scores.append(t_scores)
        idxs_and_scores = sorted(enumerate(l_scores), key=lambda x: abs(x[1]), reverse=True)
        all_idxs_and_scores.append(idxs_and_scores[:top_k])
    if threshold:
        all_scores_tensor = torch.cat(all_filtered_scores)
        threshold_val = all_scores_tensor.mean() + threshold * all_scores_tensor.std()
        for i in range(len(all_idxs_and_scores)):
            all_idxs_and_scores[i] = [(i, x) for i, x in all_idxs_and_scores[i] if abs(x) > threshold_val]
    if len(all_idxs_and_scores) > 1:
        return all_idxs_and_scores, threshold_val
    return all_idxs_and_scores[0], threshold_val


def get_tokens_and_ctx_break_idxs(
    ex: PECoReExample,
    model: AttributionModel,
    model_type: ModelTypeEnum,
    has_output_context: bool,
    ctx_break: str,
    excluded_tokens: List[str] = [],
    special_characters: List[str] = [],
) -> Tuple[List[str], List[str], List[str], List[str]]:
    input_current_tokens = tokenize_subwords(
        ex.input_current,
        model,
        model_type=model_type,
        is_target=False,
        special_tokens=excluded_tokens,
        special_characters=[],
    )
    input_context_tokens = tokenize_subwords(
        ex.input_context,
        model,
        model_type=model_type,
        is_target=False,
        special_tokens=excluded_tokens,
        special_characters=[],
    )
    output_current_tokens = tokenize_subwords(
        ex.output_current,
        model,
        model_type=model_type,
        is_target=model.is_encoder_decoder,
        special_tokens=excluded_tokens,
        special_characters=[],
    )
    output_context_tokens = None
    if has_output_context:
        output_context_tokens = tokenize_subwords(
            ex.output_context.strip(ctx_break),
            model,
            model_type=model_type,
            is_target=model.is_encoder_decoder,
            special_tokens=excluded_tokens,
            special_characters=[],
        )
    for char in special_characters:
        input_context_tokens = [tok.replace(char, " ") for tok in input_context_tokens]
        output_current_tokens = [tok.replace(char, " ") for tok in output_current_tokens]
        if has_output_context:
            output_context_tokens = [tok.replace(char, " ") for tok in output_context_tokens]
    return (
        input_current_tokens,
        input_context_tokens,
        output_context_tokens,
        output_current_tokens,
    )


def aggregate_cci_scores(
    cci_out: FeatureAttributionOutput,
    select_attributions_idx: Optional[List[int]] = None,
    attributions_aggregate_fns: Optional[List[str]] = None,
    normalize_attributions: bool = False,
) -> FeatureAttributionSequenceOutput:
    if select_attributions_idx is not None and attributions_aggregate_fns is not None:
        for idx, fn in zip(select_attributions_idx, attributions_aggregate_fns):
            cci_out = cci_out.aggregate(
                aggregator=fn,
                normalize=normalize_attributions,
                select_idx=idx,
                do_post_aggregation_checks=False,
            )
    else:
        cci_out = cci_out.aggregate(aggregator=attributions_aggregate_fns, normalize=normalize_attributions)
    return cci_out[0]


def get_cci_ranked_scores(
    model: AttributionModel,
    cci_out: FeatureAttributionSequenceOutput,
    lang_tag_offset: int,
    input_break_idx: int,
    output_break_idx: int,
    has_output_context: bool,
    top_k: int,
    threshold: float,
) -> Tuple[List[Tuple[int, float]], Optional[List[Tuple[int, float]]], float]:
    if model.is_encoder_decoder:
        cci_input_scores = cci_out.source_attributions[lang_tag_offset : lang_tag_offset + input_break_idx, 0]
        rank_scores_args = {"all_scores": cci_input_scores}
        if has_output_context:
            cci_output_scores = cci_out.target_attributions[lang_tag_offset : lang_tag_offset + output_break_idx, 0]
            rank_scores_args["all_scores"] = [cci_input_scores, cci_output_scores]
    else:
        cci_input_scores = cci_out.target_attributions[lang_tag_offset : lang_tag_offset + input_break_idx, 0]
        rank_scores_args = {"all_scores": cci_input_scores}
    cci_outputs_idxs_and_scores = None
    cci_inputs_idxs_and_scores, cci_scores_threshold = scores_to_rank(
        **rank_scores_args,
        top_k=top_k,
        threshold=threshold,
    )
    if has_output_context:
        cci_outputs_idxs_and_scores = cci_inputs_idxs_and_scores[1]
        cci_inputs_idxs_and_scores = cci_inputs_idxs_and_scores[0]
    return cci_inputs_idxs_and_scores, cci_outputs_idxs_and_scores, cci_scores_threshold


def visualize_pecore_example(
    input_context_tokens: List[str],
    output_context_tokens: List[str],
    output_current_tokens: List[str],
    output_current_contrast: str,
    example_idx: int,
    cti_metric: str,
    cti_scores_threshold: float,
    cti_tok_idx: int,
    cti_tok_score: float,
    cci_metric: str,
    cci_scores_threshold: float,
    cci_inputs_idxs_and_scores: List[Tuple[int, float]],
    cci_outputs_idxs_and_scores: Optional[List[Tuple[int, float]]],
    has_output_context: bool,
    console: Console,
) -> None:
    input_context_tokens = input_context_tokens.copy()
    output_current_tokens = output_current_tokens.copy()
    output_current_tokens[
        cti_tok_idx
    ] = f"[bold magenta]{output_current_tokens[cti_tok_idx]}({cti_tok_score:.3f})[/bold magenta]"
    for cci_tok_idx, cci_tok_score in cci_inputs_idxs_and_scores:
        input_context_tokens[
            cci_tok_idx
        ] = f"[bold green]{input_context_tokens[cci_tok_idx]}({cci_tok_score:.3f})[/bold green]"
    output_context_comment = ""
    if has_output_context:
        output_context_tokens = output_context_tokens.copy()
        for cci_tok_idx, cci_tok_score in cci_outputs_idxs_and_scores:
            output_context_tokens[
                cci_tok_idx
            ] = f"[bold green]{output_context_tokens[cci_tok_idx]}({cci_tok_score:.3f})[/bold green]"
        output_context_comment = f"\n[bold]Output context:[/bold]\t{''.join(output_context_tokens)}"
    console.print(
        f"\n#{example_idx}. (CTI |{cti_metric}| > {cti_scores_threshold:.2f}, "
        f"CCI |{cci_metric}| > {cci_scores_threshold:.2f})"
        f"\n[bold]Contextless output:[/bold]\t{output_current_contrast}"
        f"\n[bold]Current output:[/bold]\t{''.join(output_current_tokens)}"
        f"\n[bold]Input context:[/bold]\t{''.join(input_context_tokens)}{output_context_comment}"
    )


def pecore_viz():
    args = parse_args()
    model = inseq.load_model(args.model_name, args.attribution_method)
    model_has_lang_tag = has_lang_tag(model)
    gen_kwargs = {"max_new_tokens": 100}
    lang_tag_offset = 1 if model_has_lang_tag else 0
    if model_has_lang_tag:
        if args.input_lang is None or args.output_lang is None or args.model_type is None:
            raise ValueError(
                "The parameters input_lang, output_lang and model_type must be specified for multilingual models"
                "using language prefix tags."
            )
        model.tokenizer.src_lang = get_lang_from_model_type(args.model_type, args.input_lang)
        model.tokenizer.tgt_lang = get_lang_from_model_type(args.model_type, args.output_lang)
        gen_kwargs["forced_bos_token_id"] = model.tokenizer.lang_code_to_id[model.tokenizer.tgt_lang]
        args.excluded_tokens += [model.tokenizer.src_lang, model.tokenizer.tgt_lang]
    ex, has_output_context = prepare_inputs_outputs(
        model=model,
        input_txt=args.input,
        output=args.output,
        ctx_break=args.ctx_break,
        model_use_ctx_break=args.model_use_ctx_break,
        excluded_tokens=args.excluded_tokens,
        gen_kwargs=gen_kwargs,
    )
    (
        input_current_tokens,
        input_context_tokens,
        output_context_tokens,
        output_current_tokens,
    ) = get_tokens_and_ctx_break_idxs(
        ex=ex,
        model=model,
        model_type=args.model_type,
        has_output_context=has_output_context,
        ctx_break=args.ctx_break,
        excluded_tokens=args.excluded_tokens,
        special_characters=args.special_characters,
    )
    console = Console(record=True)
    visualize_procedure_details(
        args=args,
        console=console,
        input_context=ex.input_context,
        input_current=ex.input_current,
        output_context=ex.output_context,
        output_full=ex.output_full,
        has_output_context=has_output_context,
        model_has_lang_tag=model_has_lang_tag,
    )

    # Step 1: Context-sensitive Target Identification
    cti_out = model.attribute(
        ex.input_current,
        ex.output_current,
        attribute_target=True,
        step_scores=[args.cti_metric],
        contrast_sources=ex.input_full if model.is_encoder_decoder else None,
        contrast_targets=ex.output_full,
        show_progress=False,
        method="dummy",
    )[0]
    if args.show_attributions:
        cti_out.show(do_aggregation=False)
    cti_tok_idxs_and_scores, cti_scores_threshold = scores_to_rank(
        all_scores=cti_out.step_scores[args.cti_metric],
        top_k=args.cti_scores_top_k,
        threshold=args.cti_scores_std_threshold,
        tokens=[t.token for t in cti_out.target],
        excluded_tokens=args.excluded_tokens,
    )

    # Step 2: Contextual Cues Imputation
    for example_idx, (cti_tok_idx, cti_tok_score) in enumerate(cti_tok_idxs_and_scores, start=1):
        output_current_contrast, aligns, pos_start = prepare_cci_params(
            model=model,
            has_output_context=has_output_context,
            output_context=ex.output_context,
            output_current=ex.output_current if model.is_encoder_decoder else ex.output_full,
            input_current=ex.input_current,
            input_full=ex.input_full,
            impute_with_contextless_output=args.impute_with_contextless_output,
            ctx_break=args.ctx_break,
            cti_tok_idx=cti_tok_idx,
            model_has_lang_tag=model_has_lang_tag,
            model_use_ctx_break=args.model_use_ctx_break,
            force_context_aware_output_prefix=args.force_context_aware_output_prefix,
            gen_kwargs=gen_kwargs,
        )
        cci_out = model.attribute(
            ex.input_full,
            ex.output_full,
            attribute_target=True,
            show_progress=False,
            attr_pos_start=pos_start,
            attr_pos_end=pos_start + 1,
            attributed_fn=args.attributed_fn,
            method=args.attribution_method,
            contrast_sources=ex.input_current if model.is_encoder_decoder else None,
            contrast_targets=output_current_contrast,
            contrast_targets_alignments=aligns,
            contrast_force_inputs=args.contrast_force_inputs,
        )
        cci_out = aggregate_cci_scores(
            cci_out=cci_out,
            select_attributions_idx=args.select_attributions_idx,
            attributions_aggregate_fns=args.attributions_aggregate_fns,
            normalize_attributions=args.normalize_attributions,
        )
        if args.show_attributions:
            cci_out.show(do_aggregation=False)

        cci_inputs_idxs_and_scores, cci_outputs_idxs_and_scores, cci_scores_threshold = get_cci_ranked_scores(
            model=model,
            cci_out=cci_out,
            lang_tag_offset=lang_tag_offset,
            input_break_idx=len(input_context_tokens),
            output_break_idx=len(output_context_tokens) if has_output_context else None,
            has_output_context=has_output_context,
            top_k=args.cci_scores_top_k,
            threshold=args.cci_scores_std_threshold,
        )
        if not model.is_encoder_decoder:
            cti_tok_idx += len(input_current_tokens)
        visualize_pecore_example(
            input_context_tokens=input_context_tokens,
            output_context_tokens=output_context_tokens,
            output_current_tokens=output_current_tokens,
            output_current_contrast=output_current_contrast,
            example_idx=example_idx,
            cti_metric=args.cti_metric,
            cti_scores_threshold=cti_scores_threshold,
            cti_tok_idx=cti_tok_idx,
            cti_tok_score=cti_tok_score,
            cci_metric=args.attribution_method,
            cci_scores_threshold=cci_scores_threshold,
            cci_inputs_idxs_and_scores=cci_inputs_idxs_and_scores,
            cci_outputs_idxs_and_scores=cci_outputs_idxs_and_scores,
            has_output_context=has_output_context,
            console=console,
        )
    if len(cti_tok_idxs_and_scores) > 0:
        console.save_html(args.viz_path)


if __name__ == "__main__":
    pecore_viz()
